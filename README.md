180.ã€ŠDialogue-Based Relation Extractionã€‹ï¼ŒACL2020

åŸºäºå¯¹è¯çš„å…³ç³»æŠ½å–æ•°æ®é›†ï¼Œå¯¹è¯åœºæ™¯ä¸‹çš„**ä¿¡æ¯æŠ½å–**åœ¨ä¸åŒçš„ä»»åŠ¡ä¸­æœ‰ä¸åŒçš„è¡¨ç¤ºæ–¹å¼ã€‚

179.ã€ŠSpelling Error Correction with Soft-Masked BERTã€‹ï¼ŒACL2020

æ£€é”™æ¨¡å‹ï¼ˆè½»é‡çº§çš„GRUï¼‰+çº é”™æ¨¡å‹ï¼ˆé‡é‡çº§çš„BERTï¼‰ï¼Œæ•´ä½“ä¸Šæ˜¯åºåˆ—æ ‡æ³¨çš„æ€è·¯

178.ã€ŠContextual Embeddings: When Are They Worth It?ã€‹,ACL2020

è®¨è®ºä¸€ä¸ªé—®é¢˜ï¼šä»€ä¹ˆæ—¶å€™ä½¿ç”¨contextual embeddingï¼Œä»€ä¹ˆæ—¶å€™ä½¿ç”¨static embeddingï¼Ÿ

static embeddingï¼šè¯­è¨€çš„å˜åŒ–æ€§ä¸å¤šï¼Œæ•°æ®æ ‡æ³¨ä¸°å¯Œï¼›

contextual embeddingï¼šlanguage containing complex structure, ambiguous word usage, words unseen in training;

177.ã€ŠTable Search Using a Deep Contextualized Language Modelã€‹

ä»»åŠ¡ä¸Šæœ‰è¶£ï¼šè¡¨æ ¼æœç´¢ï¼›ä½†æ˜¯ï¼Œæ”¶è·å¾ˆå°ã€‚

176.ã€ŠConversational Word Embedding for Retrieval-Based Dialog Systemã€‹,ACL2020

è®­ç»ƒæ•°æ®ï¼š<post,reply>å¯¹

æ¨¡å‹ï¼šä¼ ç»Ÿè¯å‘é‡è®­ç»ƒæ¨¡å‹+å¾®ä¿®

ç”¨æ³•ï¼šå•ç‹¬ä½¿ç”¨å’Œä¼ ç»Ÿçš„Embeddingä¸€åŒä½¿ç”¨

175.ã€ŠBeyond Accuracy: Behavioral Testing of NLP Models with CheckListã€‹ï¼ŒACL2020

ä¸€ç§NLPä¸­çš„è¡Œä¸ºé©±åŠ¨æµ‹è¯•å®ç°ã€‚

174.ã€ŠIterative Memory-Based Joint Open Information Extractionã€‹ï¼ŒACL2020

åšå¼€æ”¾ä¿¡æ¯æŠ½å–çš„å·¥ä½œï¼Œä¹Ÿå°±æ˜¯å¼€æ”¾SPOæŠ½å–ç›¸å…³ã€‚ä¸»è¦åŒ…å«ä¸¤ä¸ªå·¥ä½œï¼š

ï¼ˆ1ï¼‰æ— ç›‘ç£çš„æ–¹å¼ææ•°æ®ï¼›ï¼ˆä¸€ä¸ªscore&filteræ–¹æ¡ˆï¼‰

ï¼ˆ2ï¼‰ç”Ÿæˆçš„æ–¹å¼ç”Ÿæˆå¤šä¸ªspoï¼›decoderç«¯æ¯æ¬¡åªå»ç”Ÿæˆä¸€ä¸ªspoï¼Œç„¶åå°†ç”Ÿæˆçš„spoå’ŒåŸå§‹è¾“å…¥åšèåˆï¼Œç”Ÿæˆç¬¬äºŒä¸ªspoï¼›

173.ã€ŠDIET: Lightweight Language Understanding for Dialogue Systemsã€‹

rasaå†…ç½®çš„ä¸€ä¸ªintent classificationå’Œentity extractionç»“åˆåšçš„æ¨¡å‹ã€‚æƒ³æ³•ä¸Šæ¯”è¾ƒæœ‰ç‰¹è‰²çš„æ˜¯ï¼š

ï¼ˆ1ï¼‰ç»“åˆmaskedæ–¹å¼åšè®­ç»ƒ

ï¼ˆ2ï¼‰å¯¹labelè¿›è¡Œembeddingï¼Œsimilarityä½œä¸ºlossçš„è¾“å…¥ã€‚è€Œéä¼ ç»Ÿçš„ä¸å¯¹labelåšembeddingï¼Œç›´æ¥ç®—ce lossï¼›

![173_img](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/diet.png?raw=true)

172.ã€ŠEnd-to-end LSTM-based dialog control optimized with supervised and reinforcement learningã€‹

æ¯”è¾ƒæ—©çš„æ–‡ç« äº†ï¼Œç¬¬ä¸€ç¯‡ç”¨end2endçš„æ–¹å¼åštask-orientedçš„botã€‚supervised learningå¯ä»¥ç”¨è¾ƒå°‘çš„æ•°æ®ç»™reinforcement learningæä¾›ä¸€ä¸ªå¥½çš„initial stateã€‚ä¸€èˆ¬è€Œè¨€ï¼Œç©æ³•æ˜¯å»ºç«‹åœ¨ä¸€ä¸ªå¤šåˆ†ç±»ä»»åŠ¡ä¸Šï¼Œå¯¹historyçš„åˆ©ç”¨æ˜¯å…³é”®ã€‚

171.ã€ŠEnabling Language Models to Fill in the Blanksã€‹

æå‡ºä¸€ç§é¢„è®­ç»ƒLMçš„è¾“å…¥/è¾“å‡ºæ„å»ºæ–¹å¼ã€‚è¾“å…¥æ˜¯åŒ…å«Blankçš„å¥å­ï¼Œè¾“å‡ºæ˜¯è¾“å…¥å’ŒBlankå¯¹åº”Tokençš„å¹¶ï¼ˆç”¨Answerç¬¦å·æ˜¾å¼æ‹¼æ¥ï¼‰ã€‚å¯¹æ¯”T5ï¼Œé‡‡ç”¨seq2seqï¼Œè¾“å‡ºç«¯ä¸åŒ…å«è¾“å…¥ã€‚ä¸€ç§æƒ³æ³•æ˜¯ï¼šè¾“å‡ºåŒ…å«è¾“å…¥ï¼Œåœ¨ä½œç”¨ä¸Šå¯ä»¥ç±»æ¯”seq2seqä¸­çš„encoderçš„ä½œç”¨ï¼Œå¥½å¤„æ˜¯ä¸éœ€è¦ä¸€ä¸ªå•ç‹¬çš„encoderã€‚å› æ­¤ä¹Ÿå°±èƒ½å¤Ÿè®²å¾—é€šT5åœ¨è¾“å‡ºç«¯ä¸éœ€è¦åŒ…å«è¾“å…¥ã€‚è¯¥å·¥ä½œç”¨äºæ•…äº‹ç”Ÿæˆï¼Œç”¨Blankæ›¿æ¢ä¸€æ®µæ•…äº‹æè¿°ï¼Œé‡‡ç”¨é¢„è®­ç»ƒLMç”Ÿæˆè¯¥æè¿°ï¼Œç±»æ¯”äºæ”¹å†™çš„å·¥ä½œã€‚

è‡³æ­¤ï¼Œå¯¹äºåŒ…å«Blankçš„è¾“å…¥ï¼Œè¾“å‡ºå¦‚ä½•æ„å»ºæ‰èƒ½å¾—åˆ°ä¸€ä¸ªå¥½çš„é¢„è®­ç»ƒLMï¼Œçœ‹åˆ°çš„æœ‰ä»¥ä¸‹æ–¹å¼ï¼š

ï¼ˆ1ï¼‰ä»å·¦åˆ°å³ç”ŸæˆåŸå§‹å¥å­

ï¼ˆ2ï¼‰ä»å³åˆ°å·¦ç”ŸæˆåŸå§‹å¥å­

ï¼ˆ3ï¼‰è¾“å…¥å’ŒåŸå§‹å¥å­çš„å¹¶

ï¼ˆ4ï¼‰è¾“å…¥å’ŒBlankå¯¹åº”Tokençš„å¹¶ï¼ˆç”¨Answerç¬¦å·æ˜¾å¼æ‹¼æ¥ï¼‰

ï¼ˆ5ï¼‰Blankå¯¹åº”Tokençš„å¹¶ï¼ˆç”¨Answerç¬¦å·æ˜¾å¼æ‹¼æ¥ï¼Œç±»æ¯”T5ï¼‰

170.ã€ŠA Simple Framework for Opinion Summarizationã€‹

169.ã€ŠFine-grained Fact Veriï¬cation with Kernel Graph Attention Networkã€‹ï¼ŒACL2020

ä¹‹å‰åšè¿‡ä¸€ä¸ªäº‹å®æ€§æ ¡éªŒçš„å·¥ä½œã€‚å…¸å‹çš„åœºæ™¯æ˜¯è¿™æ ·çš„ï¼šç»™å®šä¸€å¥è¯åŒ…å«å¯¹ä¸€ä¸ªäººç‰©çš„æè¿°ï¼Œå¦‚â€zhpmatrixåœ¨æ­å·å·¥ä½œï¼ŒåšNLPæ–¹å‘çš„å·¥ä½œï¼Œbalabala...â€œï¼Œä¹Ÿå°±æ˜¯è¯´è¿™å¥è¯èƒ½å¤Ÿç²¾å‡†å®šä½ä¸€ä¸ªäººç‰©ï¼šzhpmatrixï¼Œä½†æ˜¯è¿™å¥è¯ä¸­å¯èƒ½æŸä¸ªåœ°æ–¹é”™äº†ï¼ˆäº‹å®æ€§é”™è¯¯ï¼‰ï¼Œç°åœ¨è¦æ£€æŸ¥å¹¶ä¿®æ­£è¿™ä¸ªäº‹å®æ€§é”™è¯¯ã€‚

ä¸€èˆ¬çš„æ€è·¯æ˜¯ï¼šéœ€è¦ä¸€ä¸ªå‚ç…§ä¸Šä¸‹æ–‡ï¼Œè¿™ä¸ªä¸Šä¸‹æ–‡çš„å­˜åœ¨æ–¹å¼å¯ä»¥æ˜¯çŸ¥è¯†å›¾è°±ï¼Œå¯ä»¥æ˜¯éç»“æ„åŒ–çš„å¯¹è¯¥äººç‰©çš„æè¿°ç­‰ã€‚å¦‚æœæ˜¯çŸ¥è¯†å›¾è°±ï¼Œåˆ™å­˜åœ¨å®ä½“æ¶ˆæ­§çš„é—®é¢˜ï¼Œä¼šå¼•å…¥å¦å¤–ä¸€ä¸ªæ¨¡å‹ï¼›è¿™ç¯‡æ–‡ç« é‡‡ç”¨çš„æ€è·¯æ˜¯åè€…ã€‚

modelingï¼šå»ºæ¨¡ä¸ºä¸€ä¸ªå¤šåˆ†ç±»é—®é¢˜ã€‚ï¼ˆKGAæ˜¯è¿™ç¯‡å·¥ä½œçš„å†…å®¹ï¼Œä¸ªäººä¸æ˜¯ç‰¹åˆ«æ„Ÿå…´è¶£ã€‚ï¼‰

![img_169](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/fact%20verification.png?raw=true)

168.ã€ŠOn the Robustness of Language Encoders against Grammatical Errorsã€‹ï¼ŒACL2020

è®¨è®ºBERTç³»ç”¨äºä¸­æ–‡çº é”™çš„robustnessé—®é¢˜ï¼ˆè¿™ä¸ªé—®é¢˜åœ¨åŸºäºBERTçš„ç›¸å…³å·¥ä½œä¸­åŸºæœ¬éƒ½å¯ä»¥çœ‹åˆ°ï¼Œä¸è¿‡ä¸åŒä»»åŠ¡å¯¹robustnessçš„æ•æ„Ÿåº¦ä¸åŒï¼Œæ¯”å¦‚æ–‡æœ¬åˆ†ç±»ä¸€èˆ¬è®¤ä¸ºæ˜¯å¯¹äºé²æ£’æ€§ä¸æ•æ„Ÿçš„ä»»åŠ¡ï¼‰ï¼Œè§£å†³çš„æ€è·¯ï¼šadversarial learningï¼ˆå…¶å®ä¹Ÿæ˜¯ä¸€ä¸ªå¸¸è§çš„æ€è·¯äº†ï¼‰ã€‚å…·ä½“æ–¹æ³•ï¼šæ„å»ºä¸€äº›æ ·æœ¬ï¼ˆå¦‚ä½•æ„å»ºæ˜¯å…³é”®ï¼‰å’ŒåŸå§‹è®­ç»ƒæ ·æœ¬ä¸€å—è®­ç»ƒã€‚

167.ã€ŠDCR-Net: A Deep Co-Interactive Relation Network for Joint Dialog Act Recognition and Sentiment Classiï¬cationã€‹

joint learning with dialog act recognition and sentiment classification

166.ã€ŠMapping Natural Language Instructions to Mobile UI Action Sequencesã€‹ACL2020

è§£å†³çš„ä»»åŠ¡ï¼š

![img_166_0](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/acl2020.png?raw=true)

è§£å†³çš„æ–¹æ³•ï¼šï¼ˆç›¸ä¼¼çš„æ€æƒ³ï¼šå¦‚ä½•å°†åºåˆ—æ ‡æ³¨ä»»åŠ¡è½¬åŒ–ä¸ºä¸€ä¸ªseq2seqå‘¢ï¼Ÿåœ¨ä»Šå¤©ç»„é‡Œçš„åˆ†äº«ä¸­åŒæ ·æåˆ°è¿™ä¸ªè§‚ç‚¹ï¼‰

![img_166_1](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/acl2020_.png?raw=true)


165.çŸ¥è¯†é©±åŠ¨å¯¹è¯çš„ä¸¤ä¸ªåº”ç”¨å·¥ä½œï¼š

ã€ŠA Knowledge-Grounded Neural Conversation Modelã€‹

![img165_0](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/knowledge_driven_baseline_0.png?raw=true)

ã€ŠLearning to select knowledge for response generation in dialog systemsã€‹

![img165_1](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/knowledge_driven_baseline_1.png?raw=true)

164.ã€ŠA Survey on Dialog Management: Recent Advances and Challengesã€‹

å°èœœåŒ—äº¬å›¢é˜Ÿæœ€æ–°å…³äºå¯¹è¯ç®¡ç†çš„ç»¼è¿°æ–‡ç« ã€‚è®¨è®ºäº†**scalabilityï¼Œdata sparsity, training efficiency**çš„ä¸‰ä¸ªé—®é¢˜ã€‚æ€»ç»“ï¼šå·¥ä¸šç•Œè¿˜æ˜¯å†™if...else...ï¼Œç ”ç©¶ä¸Šå¯ä»¥åšä¸€äº›RLçš„æ¢ç´¢ï¼Œæœ€è¿‘èš‚èšåšå¯¹è¯çš„äººæ¥å…¬å¸äº¤æµçš„æ—¶å€™ï¼Œè¯´ä»–ä»¬å°±åœ¨åšç±»ä¼¼å·¥ä½œã€‚

163.ã€ŠStructBERT:Incorporating Language Structures into Pre-training for Deep Language Understandingã€‹,ICLR2020

è¾¾æ‘©é™¢çš„å…³äºbertçš„å·¥ä½œï¼Œä½œä¸ºåŸºç¡€æ¨¡å‹åœ¨æ¨å¹¿ã€‚

162.ã€ŠEnriched Pre-trained Transformers for Joint Slot Filling and Intent Detectionã€‹

slot filling and intent classificationåŒæ—¶åšï¼ŒåŸºæœ¬æ˜¯åœ¨è¾¾æ‘©é™¢ä¹‹å‰çš„å·¥ä½œåŸºç¡€ä¸ŠåŠ äº†ä¸€äº›ä¸œè¥¿ã€‚

![img_162](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/Enriched%20Pre-trained%20Transformers%20for%20Joint%20Slot%20Filling%20and%20Intent%20Detection.png?raw=true)


161.ã€ŠLook at the First Sentence: Position Bias in Question Answeringã€‹

éå¸¸æœ‰æ„æ€çš„æ–‡ç« ï¼Œè®¨è®ºäº†QAä¸­çš„positional biasé—®é¢˜ã€‚

160.ã€ŠCode-Switching for Enhancing NMT with Pre-Specified Translationã€‹ï¼ŒNAACL2019

å¾ˆå®ç”¨çš„å·¥ä½œã€‚åˆ©ç”¨ç”¨æˆ·è¯å…¸å’Œç”µå•†æœ¯è¯­åº“æå‡ç¿»è¯‘è´¨é‡ã€‚ç®€å•æ¥è¯´ï¼Œä¹‹å‰çš„æ–¹å¼æ˜¯ç”¨ä¸€ä¸ªç‰¹æ®Šç¬¦å·å ä½ï¼Œç¿»è¯‘å¯¹åº”çš„è¯ï¼›ç‰¹æ®Šç¬¦å·å’ŒåŸå§‹ä¸Šä¸‹æ–‡åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¼šç ´ååŸå§‹çš„è¯­ä¹‰ä¿¡æ¯ã€‚è¿™é‡Œé‡‡ç”¨çš„æ˜¯å¦å¤–ä¸€ç§æ–¹å¼(å‡è®¾ä¸­è‹±ç¿»è¯‘ï¼Œå…¶å®æ˜¯å¦å¤–ä¸€ç§å ä½ï¼Œä¸è¿‡semanticä¸Šä¼¼ä¹æ›´åŠ åˆç†ä¸€äº›)ï¼š

åŸæ–‡ï¼šç—…è¶Šæ¥è¶Šå‰å®³

ä¸­é—´åŸæ–‡ï¼šsickè¶Šæ¥è¶Šå‰å®³

ç¿»è¯‘ç»“æœï¼šsick is worse

æ€»ç»“ï¼š**æœºå™¨ç¿»è¯‘ä¸­çš„å¹²é¢„æœºåˆ¶**æ˜¯ä¸€ä¸ªå°æ–¹å‘ï¼Œéå¸¸å…·æœ‰å®ç”¨ä»·å€¼ï¼Œç›¸å…³å·¥ä½œåº”è¯¥æœ‰ä¸å°‘ã€‚

159.ã€ŠHow Does NLP Benefit Legal System: A Summary of Legal Artificial
Intelligenceã€‹ACL2020

æ¯”èµ›ï¼Œç›¸å…³æ–‡ç« ï¼Œè¿˜æœ‰è¿™ç¯‡ï¼Œæ³•å¾‹é¢†åŸŸçš„PRç¨¿ï¼Ÿ

158.ã€ŠBLEU Neighbors: A Reference-less Approach to Automatic Evaluationã€‹

æ¢³ç†äº†ç›¸å…³è¯„ä»·æŒ‡æ ‡çš„ä¸€äº›å·¥ä½œã€‚

157.ã€ŠLightPAFF: A Two-Stage Distillation Framework for Pre-training and finetuningã€‹

è¿™ç¯‡æ–‡ç« çš„æŠ€æœ¯æ€è·¯é£æ ¼å’Œä¹‹å‰è¯»è¿‡çš„ä¸€ç¯‡å…³äºBTçš„æ–‡ç« å¾ˆæ˜¯ç±»ä¼¼ã€‚

156.ã€ŠCoach: A Coarse-to-Fine Approach for Cross-domain Slot Fillingã€‹,ACL2020

æ–‡ä¸­çš„template regularizationæ¯”è¾ƒæœ‰æ„æ€ã€‚æ•´ä½“ä¸Šçš„æ€è·¯æ˜¯coarse-to-fineï¼Œæ˜¯å®ä½“è¯†åˆ«ä¸­çš„æ ‡å‡†èŒƒå¼ï¼ˆæ¯”å¦‚é€šå¸¸ç¬¬ä¸€æ­¥æ˜¯è¾¹ç•Œé¢„æµ‹ï¼Œç¬¬äºŒæ­¥æ˜¯ç±»å‹é¢„æµ‹ï¼‰ï¼›

155.ã€ŠLearning to Rank with BERT in TF-Rankingsã€‹

æœ€è¿‘åšé—®ç­”ï¼Œä¸€ä¸ªæ¯”è¾ƒgeneralçš„æ¡†æ¶æœ€åæ˜¯ä¸€ä¸ªrankingæ¨¡å‹ï¼Œè¿™ç¯‡æ–‡ç« æ²¡æœ‰å•ç‹¬å°†rankingæ¨¡å‹å‰¥ç¦»å‡ºæ¥ï¼Œè€Œæ˜¯å’ŒBERTä¸€å—modelingï¼Œæ€è·¯ä¸Šæœ‰å¯å‘æ€§ã€‚

![img155](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/bert_with_tf_ranking.png?raw=true)

154. èåˆè¯å…¸çš„ä¿¡æ¯ç”¨äºNERçš„ä¸‰ç¯‡å·¥ä½œï¼Œé»„è±èè€å¸ˆç»„æ„Ÿè§‰å¯¹è¿™ä¸ªtopicå¾ˆæ„Ÿå…´è¶£ã€‚å»å¹´ACL2019æœ‰ç¯‡PU Learningæçš„ã€‚

ã€ŠChinese NER Using Flat-Lattice Transformerã€‹ACL2020

æŠŠLattice LSTMä¸­å¯¹è¯çš„åˆ©ç”¨æ‹å¹³ã€‚

ã€ŠSimplify the Usage of Lexicon in Chinese NERã€‹ï¼ŒACL2020

å·¥ä½œåœ¨æ€è·¯ä¸Šç±»ä¼¼ZENã€‚

ç›¸å…³å·¥ä½œï¼š

ã€ŠEnhancing Pre-trained Chinese Character Representation with Word-aligned Attentionã€‹

153.ã€ŠLearning to Classify Intents and Slot Labels Given a Handful of Examplesã€‹

è§£å†³Chatbotä¸­çš„low resource settingä¸‹çš„é—®é¢˜ã€‚æ ¸å¿ƒæ€è·¯ï¼šMAMLå’ŒåŸå‹ç½‘ç»œ+pre-trained modelç”¨äºæ„å›¾åˆ†ç±»å’Œæ§½å¡«å……ã€‚

152.ã€ŠDistilling Knowledge for Fast Retrieval-based Chat-botsã€‹SIGIR2020

å…³äºçŸ¥è¯†è’¸é¦çš„ï¼ŒæŠŠBERTçš„çŸ¥è¯†è’¸é¦åˆ°BiLSTMä¸­ã€‚å…¶ä¸­æœ‰ä¸¤ä¸ªå°Trickï¼Œä¸€ä¸ªæ˜¯å¸¦ä¸ŠåŸå§‹çš„æ ‡ç­¾ï¼›å¦ä¸€ä¸ªæ˜¯ç®—MSEï¼›éƒ½æ˜¯Lossç«¯çš„ä¼˜åŒ–ã€‚æœ€ç®€å•çš„å½¢å¼æ˜¯åªè¦softæ ‡ç­¾+ceæŸå¤±ã€‚ï¼ˆè‡ªå·±ç”¨12å±‚BERTè’¸6å±‚ï¼Œè’¸Albertã€‚ï¼‰

151.ã€ŠSKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysisã€‹ï¼ŒACL2020ï¼ŒBaidu

æ ¸å¿ƒæ€æƒ³ï¼šå¯¹aspectå’Œsent wordè¿›è¡Œmaskã€‚

å¥—è·¯ï¼šBaiduçš„åŒå­¦è¯´ï¼Œæˆ‘ä»¬èƒ½é¢„è®­ç»ƒï¼Œæƒ³è¦å•¥ï¼Œå°±å¯¹å•¥è¿›è¡Œmaskï¼Œä½ çœ‹ï¼Œåˆ·äº†æƒ…æ„Ÿçš„sotaå§ï¼Ÿ

æˆ‘ï¼šé¼“æŒã€‚ã€‚

![151_img](https://mmbiz.qpic.cn/mmbiz_jpg/uYIC4meJTZ0vad7YcUGC4WTAgya8zHRvsPk3dgP8lN1cJu8EnTXnEh0iahbGCQGL0XoItBF4aWZEfZXzaSAhZicg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

150.ã€ŠFAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevanceã€‹

FAQæœ€è¿‘çš„ä¸€ç¯‡paperï¼Œæ€æƒ³ä¸Šæ˜¯å®Œæ•´çš„ã€‚

Query-Questionï¼šç›¸ä¼¼æ€§å»ºæ¨¡

Query-Answerï¼šç›¸å…³æ€§å»ºæ¨¡

æ¨¡å‹èåˆï¼šåˆ†æ•°åŠ æƒï¼ˆä¸€äº›åŸºäºé•¿åº¦çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼‰

149.ã€ŠNeural Architectures for Named Entity Recognitionã€‹

LSTM+CRFçš„ç»å…¸paperï¼Œé‡Œè¾¹è¿˜èŠåˆ°ä¸€ç§transition-based alg,ç±»ä¼¼äºä¾å­˜åˆ†æçš„shift-reduced algã€‚ï¼ˆä¹‹å‰åšæŠŠä¾å­˜ä¿¡å·encodeè¿›bertçš„æ—¶å€™ï¼Œç¬¬ä¸€æ¬¡äº†è§£åˆ°shift-reducedçš„æ¦‚å¿µï¼‰

148.ã€ŠDialogue-Based Relation Extractionã€‹

å¯¹è¯æ•°æ®ä¸­åšå…³ç³»æŠ½å–çš„æ•°æ®é›†ã€‚

147.ã€ŠA Survey of Text Clustering Algorithmsã€‹,Charu C. Aggarwal, IBM

+ K-means(äºŒåˆ†K-meansã€K-means++)å’Œå±‚æ¬¡èšç±»æ˜¯éå¸¸generalçš„clusteringç®—æ³•
+ LDA+pLSA
+ Embedding-based(One-Hot, BOW, Word2Vec,TF-IDF,Sentence-BERT)
+ Online Clustering(è¾ƒé™Œç”Ÿ)
+ å…¶ä»–


146.ã€ŠA Novel Hierarchical Binary Tagging Framework for Relational Triple Extractionã€‹

![img146](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/hbt.png?raw=true)


145.ã€ŠPALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generationã€‹ï¼ŒACL2020

![img145](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/palm.png?raw=true)

144.PU Learning

ã€ŠLearning Classifiers from Only Positive and Unlabeled Dataã€‹ï¼Œ2008

ã€ŠAn Evaluation of Two-Step Techniques for Positive-Unlabeled Learning in Text Classificationã€‹ï¼Œ2014

ä¸¾ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼šè®­ç»ƒäºŒåˆ†ç±»çš„æ—¶å€™ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ï¼Œéœ€è¦æ ‡æ³¨æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ï¼›ç°åœ¨ä¸äº†ï¼Œåªæ ‡æ³¨æ­£æ ·æœ¬å°±OKäº†ï¼ŒæŠŠå‰©ä¸‹çš„å½“åšè´Ÿç±»ï¼ˆè™½ç„¶å‰©ä¸‹çš„å¯èƒ½åŒ…å«æ­£ç±»ï¼‰ã€‚ä¸€æ–¹é¢å¯ä»¥å‡å°æ ‡æ³¨çš„éš¾åº¦ï¼Œå¦ä¸€æ–¹é¢æ˜¯å¯¹æ•°æ®åç½®çš„åˆ©ç”¨ã€‚æ•´ä½“ä¸Šï¼Œå¯ä»¥æ”¾åœ¨åŠç›‘ç£çš„è§†è§’ä¸‹æ¥çœ‹ã€‚


143.ã€ŠCurate and Generate: A Corpus and Method for Joint Control of Semantics and Style in Neural NLGã€‹

å¦‚ä½•æ„é€ NLGçš„æ•°æ®ï¼Ÿ

142.ã€ŠOverestimation of Syntactic Representation in Neural Language Modelsã€‹ï¼ŒACL2020

140.ã€ŠNatural Perturbation for Robust Question Answeringã€‹

139.ã€ŠA Joint Model for Question Answering and Question Generationã€‹

138.Queryæ”¹å†™ï¼šã€ŠLearning to Paraphrase for Question Answeringã€‹ï¼ŒEMNLP2017

137.ã€ŠReading Wikipedia to Answer Open-Domain Questionsã€‹,Danqi Chenï¼Œåº”è¯¥è¿˜æœ‰ä¸å°‘ç©å„¿æ³•

136.ã€ŠA Survey on Dialogue Systems: Recent Advances and New Frontiersã€‹,JDçš„æ–‡ç« ï¼Œç®€å•æ¸…æ™°

135.å•æ­£ä¸œä¸¤ç¯‡å…³äºç¥ç»+ç¬¦å·çš„å·¥ä½œï¼š

https://arxiv.org/pdf/1512.00965.pdf

ç”¨åœ¨å¯¹è¯ç³»ç»Ÿï¼Œæå‡é²æ£’æ€§ï¼š

ã€ŠNeural symbolic machines: Learning semantic parsers on freebase with weak supervisionã€‹

ã€ŠNeural-symbolic machine learning for retrosynthesis and reaction prediction. Chemistryâ€“A European Journalã€‹

ã€ŠCoupling Distributed and Symbolic Execution for Natural Language Queriesã€‹

ã€ŠNEURAL SYMBOLIC READER: SCALABLE INTEGRATION OF DISTRIBUTED AND SYMBOLIC REPRESENTATIONS FOR READING COMPREHENSIONã€‹ï¼ŒICLR2020ï¼ˆåŸæ¥è¿™ä¸ªæ–¹å‘çœŸçš„æœ‰ä¸€äº›å·¥ä½œåœ¨åšã€‚ï¼‰

134.ã€ŠFrom Machine Reading Comprehension to Dialogue State Tracking: Bridging the Gapã€‹

æ ¸å¿ƒæ–¹æ³•ï¼šå°†DSTä»»åŠ¡ç”¨MRCçš„æ–¹å¼modelingï¼Œå‰©ä¸‹çš„å°±æ˜¯MRCçš„ä¼˜ç‚¹äº†ã€‚

133.ã€ŠCrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Datasetã€‹ï¼ŒACL2020

æœ€è¿‘å‡ºçš„å¤§è§„æ¨¡ï¼Œè·¨é¢†åŸŸï¼ˆé…’åº—ï¼Œé¤é¦†ç­‰ï¼‰ï¼Œä»»åŠ¡å‹å¯¹è¯æ•°æ®é›†ã€‚é™¤äº†ç»™å‡ºä¸€ä¸ªæ•°æ®é›†ä¹‹å¤–ï¼ŒåŒæ—¶å°†ä¸€ä¸ªå¯¹è¯ç³»ç»Ÿåˆ†ä¸ºäº”ä¸ªéƒ¨åˆ†ï¼šuser simulator, nlu, dst, dpl, nlg.å…¶ä¸­user simulatoræœ‰ç‰¹è‰²ã€‚æ­¤å¤–ï¼Œåœ¨æ¯ä¸ªstageï¼Œéƒ½ç»™å‡ºäº†ä¸€ä¸ªå®ç°ã€‚

è¡¥å……ï¼šã€ŠMuTual: A Dataset for Multi-Turn Dialogue Reasoningã€‹ACL2020

åŸºäºä¸­å›½é«˜è€ƒè‹±è¯­å¬åŠ›ï¼Œéœ€è¦é€»è¾‘å’Œæ¨ç†ï¼ˆBERTæ¯”äººå·®å¾ˆå¤šï¼‰ã€‚éš¾åº¦æ¯”Ubuntu Dialogue Corpusé«˜å¾ˆå¤šï¼ˆå·²ç»è¢«BERTæ‹¿ä¸‹ï¼‰ã€‚

132.æœ€è¿‘çš„Reviewæ–‡ç« æ„Ÿè§‰æ¯”è¾ƒå¤šï¼Œæ¢³ç†å¦‚ä¸‹ï¼š

æ–‡æœ¬åˆ†ç±»ï¼šã€ŠDeep Learning Based Text Classification: A Comprehensive Reviewã€‹

å…³ç³»æŠ½å–ï¼šã€ŠMore Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extractionã€‹

NER: ã€ŠA Survey on Deep Learning for Named Entity Recognitionã€‹

é¢„è®­ç»ƒæ¨¡å‹ï¼šã€ŠPre-trained Models for Natural Language Processing: A Surveyã€‹

çŸ¥è¯†å›¾è°±ï¼šã€ŠA Survey on Knowledge Graphs: Representation, Acquisition and Applicationsã€‹

æœºå™¨ç¿»è¯‘ï¼šã€ŠNeural Machine Translation: Challenges, Progress and Futureã€‹

Chatbotï¼šã€ŠRecent Advances and Challenges in Task-oriented Dialog Systemã€‹

æ–‡æœ¬é£æ ¼è¿ç§»ï¼šã€ŠReview of Text Style Transfer Based on Deep Learningã€‹

131.ã€ŠDeep Learning Based Text Classification: A Comprehensive Reviewã€‹,42é¡µï¼Œæœ€æ–°ç»¼è¿°ï¼Œæˆ‘çœ‹ä¸åŠ¨äº†...

130.ã€ŠPoly-Encoders: Architectures And Pre-training Strategies For Fast and Accurate Multi-Sentence Scoresã€‹,ICLR2020

ä¸Šç¯‡æ–‡ç« çš„followerï¼šã€ŠSentence-BERT: Sentence Embeddings using Siamese BERT-Networksã€‹è¿™ç¯‡æ–‡ç« ï¼Œå¾ˆæ—©å°±æµè§ˆåˆ°ï¼Œå½“æ—¶è§‰å¾—ä¸å°±æ˜¯Encoderç«¯æ¢æˆBERTäº†å—ï¼Ÿ...ä¸è¿‡çœ‹äº†ICLR2020çš„é‚£ç¯‡ï¼ŒFAIRå‡ºå“çš„ï¼Œæˆ‘éƒ½ä¸æ¸…æ¥šå•¥æ˜¯å¥½ï¼Œå•¥æ˜¯ä¸å¥½äº†ï¼Ÿ

æœ€è¿‘åšQAï¼Œé‡æ–°ç¿»åˆ°è¿™ä¸¤ç¯‡æ–‡ç« ï¼Œæœ‰ä¸€äº›æ–°çš„æƒ³æ³•ã€‚

129.ã€ŠDistantly Supervised Named Entity Recognition using Positive-Unlabeled Learningã€‹

æ ¸å¿ƒè§‚ç‚¹ï¼šåªç”¨è¯å…¸åšå‘½åå®ä½“è¯†åˆ«

128.ã€ŠCoreference Resolution as Query-based Span Predictionã€‹ï¼ŒACL2020

æçºªä¸ºç¬¬ä¸‰ç¯‡ç”¨é—®ç­”çš„æ€è·¯åšçš„å·¥ä½œï¼Œä¸¤ä¸ªç›¸å…³æ•°æ®é›†çš„SOTAã€‚ç¬¬ä¸€æ­¥ï¼šæ‰¾å‡ºå€™é€‰å®ä½“ï¼›ç¬¬äºŒæ­¥ï¼šQueryæ„å»ºã€‚åŒ…å«å€™é€‰å®ä½“çš„å¥å­ï¼Œç”¨ç‰¹æ®Šæ ‡è¯†ç¬¦æ ‡è¯†å‡ºå€™é€‰å®ä½“ã€‚è¾“å…¥ä¸ºQuery+Contextï¼Œè¾“å‡ºä¸ºå…¶ä»–å®ä½“çš„ä½ç½®ã€‚å’Œä¹‹å‰çš„å·¥ä½œç±»ä¼¼ï¼Œä¸€ä¸ªæ ·æœ¬ä»æ—§éœ€è¦å¤šæ¬¡inferenceå®Œæˆè®­ç»ƒå’Œæµ‹è¯•ã€‚

ç›¸å…³æ–‡ç« ï¼š

ï¼ˆ1ï¼‰ã€ŠDice Loss for Data-imbalanced NLP Tasksã€‹

ï¼ˆ2ï¼‰ã€ŠA Uniï¬ed MRC Framework for Named Entity Recognitionã€‹

ï¼ˆ3ï¼‰ã€ŠScaling Up Open Tagging from Tens to Thousands: Comprehension Empowered Attribute Value Extraction from Product Titleã€‹

è¡¥å……ï¼š

ã€ŠEvent Extraction by Answering (Almost) Natural Questionsã€‹ï¼Œè·¯å­ä¸€è‡´ï¼Œç”¨åœ¨äº‹ä»¶æŠ½å–ä¸Šå¾ˆæ­£å¸¸ã€‚

127.æ¨¡å‹æ ¡å‡†

logitå°±æ˜¯ç½®ä¿¡åº¦ï¼Œå°±æ˜¯æ¦‚ç‡ï¼Ÿnot for all.

æ¦‚ç‡æ ¡å‡†è§£é‡Šçš„ç›¸å¯¹ç³»ç»Ÿçš„æ–‡ç« ï¼šhttps://zhuanlan.zhihu.com/p/90479183

sklearnçš„ä»‹ç»ï¼šhttps://scikit-learn.org/stable/auto_examples/calibration/plot_calibration.html#sphx-glr-auto-examples-calibration-plot-calibration-py

æ ¸å¿ƒè§‚ç‚¹ï¼š

+ å¯¹äºSVMï¼Œæ˜¯åŸºäºmarginçš„è®­ç»ƒï¼Œæ²¡æœ‰prob

+ Platt Scaling:åŸå§‹æ¨¡å‹ä¹‹å¤–ï¼Œé‡æ–°è®­ç»ƒä¸€ä¸ªæ¨¡å‹(LR)

+ Isotonic Regression(ä¿åºå›å½’)ï¼šæ›²çº¿æ‹Ÿåˆ

+ æ¦‚ç‡æ ¡å‡†æ–¹æ³•çš„è¯„ä¼°

å…³äºé¢„è®­ç»ƒNLPæ¨¡å‹çš„å·¥ä½œï¼šã€ŠCalibration of Pre-trained Transformersã€‹ï¼ˆä¸ªäººå›´ç»•è¿™å—ä¹Ÿåšäº†ä¸€äº›å·¥ä½œï¼‰

ã€ŠOn Calibration of Modern Neural Networksã€‹

ã€ŠTraining Confidence-Calibrated Classifiers For Detection Out-Of-Distribution Samplesã€‹,ICLR2018ï¼ˆè¿˜æ²¡è¯»..ï¼‰

ã€ŠDistance-Based Learning from Errors for Confidence Calibrationã€‹ICLR2020ï¼ˆè¿˜æ²¡è¯»..ï¼‰

ã€ŠPosterior Calibrated Training on Sentence Classification Tasksã€‹ACL2020ï¼ˆè¿˜æ²¡è¯»..ï¼‰

126.é˜…è¯»ç†è§£ä¸­çš„multi-spanå·¥ä½œ

å¯å‘ï¼šMRCä¹Ÿæ˜¯ä¸€ä¸ªç»å…¸çš„è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ï¼ŒåŒæ—¶åŒ…æ‹¬å¾ˆå¤šå­ä»»åŠ¡ã€‚åœ¨multi-spançš„é—®é¢˜è®¾å®šä¸‹ï¼Œæœ‰ä¸€äº›æœ‰æ„æ€çš„æƒ³æ³•ã€‚

ã€ŠTag-based Multi-Span Extraction in Reading Comprehensionã€‹ï¼ŒBIOæ ‡æ³¨+**å¤šç§è§£ç ç­–ç•¥å¯¹æ¯”**

ã€ŠA Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoningã€‹

125.ã€ŠWell-Read Students Learn Better: On the Importance of Pre-training Compact Modelsã€‹

å›´ç»•BERTåšæ¨¡å‹å‹ç¼©çš„å·¥ä½œï¼Œå¾ˆæ‰å®ã€‚é™„å¸¦Jacob Devlinæœ€è¿‘åœ¨ä¸€ä¸ªtalkä¸Šçš„å…³äºæ¨¡å‹è’¸é¦çš„æƒ³æ³•ï¼š

![img125](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/model_distill.png?raw=true)


124.æŒ‡ä»£æ¶ˆè§£çš„è®ºæ–‡ï¼ˆéƒ¨é—¨[å¤§ä½¬](https://github.com/jerrychen1990)çš„ç¬”è®°ï¼‰

End-to-end Neural Coreference Resolution:baselineç‰ˆæœ¬ï¼Œå°†é¢„æµ‹spanä¸é¢„æµ‹ä¸¤ä¸ªspanæ˜¯å¦åŒæŒ‡ä»£æ”¾åœ¨ä¸€ä¸ªä»»åŠ¡é‡Œè®­ç»ƒã€‚**åˆ©ç”¨å‰ªæçš„æ–¹å¼é¿å…è¿‡å¤§å¤æ‚åº¦çš„span pairæœç´¢**

Higher-order Coreference Resolution with Coarse-to-ï¬ne Inferenceï¼šé™¤äº†åˆ©ç”¨pair-wiseçš„ä¿¡æ¯ï¼Œåˆ©ç”¨æ•´ä¸ªclusterçš„ä¿¡æ¯åšspan matching

BERT for Coreference Resolution: Baselines and Analysisï¼šåœ¨ä¸Šä¸€ç¯‡è®ºæ–‡çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨bertä»£æ›¿åŸæœ‰çš„ç‰¹å¾æå–å™¨

123.ã€ŠInformation Leakage in Embedding Modelsã€‹

![img123](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/Information%20Leakage%20in%20Embedding%20Models.png?raw=true)

122.ã€Šword2vec, node2vec, graph2vec, X2vec: Towards a Theory of Vector Embeddings of Structured Dataã€‹

121.ã€ŠAliCoCo: Alibaba E-commerce Cognitive Concept Netã€‹

é˜¿é‡Œè®¤çŸ¥å›¾è°±çš„æ„å»ºå·¥ä½œï¼ŒPAKDD2020ã€‚

ç›¸å…³å·¥ä½œï¼š å“ˆå·¥å¤§çš„å¤§è¯æ—ï¼Œç›¸å…³çš„å·¥ä½œä¹Ÿæ˜¯å¾ˆèµã€‚æœ€è¿‘æ”¾å‡ºäº†å¾ˆå¤§ä¸€æ‰¹[æ•°æ®](https://mp.weixin.qq.com/s?__biz=MzIxMjAzNDY5Mg==&mid=2650797234&idx=1&sn=a2791160d643a04ace67e4630046b6b9&chksm=8f476159b830e84ff25c9a62b74a3074d8c8c4b57dfbec04faa689ee4be74e376ce900f6cee3&token=711471654&lang=zh_CN%23rd)ã€‚

120.ã€ŠImproving Named Entity Recognition for Chinese Social Media with Word Segmentation Representation Learningã€‹

Joint Learingï¼š NER+CWSï¼ˆåº”è¯¥ä¹Ÿå±äºæ¯”è¾ƒæ—©çš„å·¥ä½œï¼‰

119.ã€ŠCharNER: Character-Level Named Entity Recognitionã€‹

æ¯”è¾ƒæ—©æœŸçš„æ–‡ç« äº†ã€‚è¯æ˜äº†å¯¹äºNERä»»åŠ¡æ¥è¯´ï¼Œä¸åˆ†è¯ä¹Ÿæ˜¯OKçš„ã€‚å°¤å…¶å¯¹äºä¸­æ–‡å’Œæ—¥æ–‡ã€‚

118.ã€ŠComprehensive Named Entity Recognition on CORD-19 with Distant or Weak Supervisionã€‹

è¿™ç¯‡æ–‡ç« å¼•ç”¨äº†ä¸€äº›æ–‡ç« ï¼Œç”¨è¯å…¸åŠ æŒNERä»»åŠ¡ï¼ŒJiongboçš„å¥½å‡ ç¯‡æ–‡ç« ã€‚

117.ä¸å¹³è¡¡é—®é¢˜çš„ä¸¤ç¯‡paper

ã€ŠClass-Balanced Loss Based on Effective Number of Samplesã€‹ï¼ŒCVPR2019

ã€ŠDice Loss for Data-imbalanced NLP Tasksã€‹ï¼ŒNLPé—®é¢˜ä¸Šï¼ˆä¸»è¦æ˜¯NERå’ŒMRCä»»åŠ¡ä¸Šçš„ä¸å¹³è¡¡ï¼‰ï¼Œè¿™ç¯‡æ–‡ç« é’ˆå¯¹NERå’ŒMRCä»»åŠ¡ä¸­çš„æ ‡ç­¾ä¸å¹³è¡¡é—®é¢˜ï¼Œæ¯”è¾ƒäº†åŒ…æ‹¬ceï¼Œweighted ceï¼Œfocal lossï¼Œdice lossç­‰å…±è®¡6ç§lossåœ¨å¤šä¸ªç†è§£ä»»åŠ¡ä¸Šçš„æ•ˆæœã€‚ï¼ˆæˆ‘çœŸçš„å¾ˆå°‘å¬è¯´focal lossåœ¨ä¸€äº›ä»»åŠ¡ä¸Šå¸®åŠ©å¾ˆå¤§ï¼Œä»2017å¹´åšä¸€ä¸ªæ¯”èµ›çš„æ—¶å€™å°±ç”¨ï¼Œåˆ°2020å¹´äº†ï¼Œæ¯æ¬¡å‡†å¤‡ç”¨çš„æ—¶å€™éƒ½è¦å˜€å’•ä¸€ä¸‹ï¼Œä»¥åä¸å†ç”¨äº†ã€‚ï¼‰

116.ã€ŠPairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articlesã€‹

BERTç³»ç”¨äºdocåˆ†ç±»ã€‚

115. Multi-Hop MRCï¼ˆè¿™æ˜¯ä¸€ä¸ªæœ‰æ„æ€çš„é—®é¢˜ï¼‰

ã€ŠCognitive Graph for Multi-Hop Reading Comprehension at Scaleã€‹ï¼Œä¸»è¦æ€æƒ³æ˜¯ï¼šBERT+GNNã€‚

ã€ŠBuilding Dynamic Knowledge Graphs From Text Using Machine Reading Comprehensionã€‹ï¼ŒICLR2019

é—®é¢˜çš„settingå¦‚ä¸‹ï¼š

![115](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/multi-hop-mrc.png?raw=true)

114.ã€ŠOne for All: Neural Joint Modeling of Entities and Eventsã€‹

ä¿¡æ¯æŠ½å–é¢†åŸŸçš„"å®—æ•™ä¹‹äº‰"ï¼špipeline v.s. joint(äºŒè€…å„æœ‰ä¼˜ç¼ºç‚¹ï¼Œåœ¨è‡ªå·±çš„å®è·µä¸­ï¼Œä¸»è¦çœ‹ä¸ªäººçš„æŠ€æœ¯å“å‘³äº†ï¼Œè‡ªå·±æ¯”è¾ƒå–œæ¬¢åè€…ï¼Œä½†æ˜¯è¿˜æ˜¯è¦ç»¼åˆè€ƒè™‘æ•°æ®ï¼Œä»»åŠ¡ç­‰å„ä¸ªæ–¹é¢çš„å› ç´ )

ä¸ªäººæ¯”è¾ƒå–œæ¬¢çš„å·¥ä½œï¼šä¸€ä¸ªæ¨¡å‹åšäº‹ä»¶æŠ½å–ï¼ŒåŒ…æ‹¬triggerè¯†åˆ«ï¼Œå®ä½“è¯†åˆ«ï¼Œå…³ç³»è¯†åˆ«ã€‚ï¼ˆå¦å¤–å…³äºä¸€ä¸ªæ¨¡å‹çš„åšæ³•ï¼Œä¸ä»…åœ¨taggingæ–¹å¼ä¸Šåšï¼Œä¹Ÿå¯ä»¥é€šè¿‡å…±äº«encoderåœ¨classifierä¸Šåšï¼Œinferenceçš„æ—¶å€™ä»æ—§å¯ä»¥æ‹†æˆå¤šä¸ªæ¨¡å‹ï¼Œæœ¬è´¨åœ¨äºshared encoderå’Œmulti-taskï¼‰

![114](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/One%20for%20All_Neural%20Joint%20Modeling%20of%20Entities%20and%20Events.png?raw=true)

113.ã€ŠExploring Pre-trained Language Models for Event Extraction and Generationã€‹

å·¥ä½œï¼špipeline+ä¸€ç§æ•°æ®å¢å¼ºæ–¹æ¡ˆ

![113](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/Exploring%20Pre-trained%20Language%20Models%20for%20Event%20Extraction%20and%20Generation.png?raw=true)

112.ã€ŠFelix: Flexible Text Editing Through Tagging and Insertionã€‹

å’ŒEMNLP2019çš„è¿™ç¯‡ã€ŠEncode, Tag, Realize: High-Precision Text Editingã€‹ç±»ä¼¼ï¼Œä¸Šæ–‡çš„æ¯”è¾ƒå¯¹è±¡æ­£æ˜¯è¿™ç¯‡æ–‡ç« ã€‚

111.ã€ŠPrior Knowledge Driven Label Embedding for Slot Filling in Natural Language Understandingã€‹ä¿å‡¯è€å¸ˆç»„çš„æ–‡ç« 

æ ¸å¿ƒè§‚ç‚¹ï¼šèåˆå…ˆéªŒçŸ¥è¯†åˆ°slot fillingä»»åŠ¡ä¸­ï¼Œå…ˆéªŒçŸ¥è¯†åˆ†ä¸ºä¸‰ç±»ï¼š

atomic concepts:åŸå­æ¦‚å¿µï¼Œæ¯”å¦‚ä¸€ä¸ªslotç±»å‹å¯ä»¥æè¿°ä¸ºå…¶ä»–å¤šä¸ªå­ç±»å‹

slot description:å°±æ˜¯slotç±»å‹çš„æè¿°äº†

slot exemplars: å°±æ˜¯slotæœ¬èº«å•¦

æ•´ä½“ä¸Šçš„æ–¹å¼å°±æ˜¯ç¡®å®šè¦ç”¨çš„å…ˆéªŒçŸ¥è¯†åï¼Œåšencodeï¼Œèåˆè¿›slot fillingä»»åŠ¡ä¸­ã€‚å‰ä¸¤ç§å…ˆéªŒçŸ¥è¯†éœ€è¦ä¸“å®¶å‚ä¸ã€‚

110.ã€ŠAdaptive Name Entity Recognition under Highly Unbalanced Dataã€‹

æå‡robustnessï¼šå…³äºnoiseå’Œmissing data

![img110](https://wx2.sinaimg.cn/mw690/aba7d18bly1gd5yvm5eyvj210y0ju489.jpg)

109.ã€ŠAUC-Maximized Deep Convolutional Neural Fields For Sequence Labelingã€‹,ICLR2016

ã€ŠAUCpreD: proteome-level protein disorder prediction by AUC-maximized deep convolutional neural fieldsã€‹ï¼ŒECCB2016

åŒä¸€ç¯‡æ–‡ç« ã€‚æ ¸å¿ƒè§‚ç‚¹(ç”¨äºsequence labelingä»»åŠ¡)ï¼š

**The widely-used training methods, such as maximum-likelihood and maximum labelwise accuracy, do not work well on imbalanced data.by directly maximizing the empirical Area Under the ROC Curve (AUC), which is an unbiased measurement for imbalanced data.**


108.ã€ŠParallel sequence tagging for concept recognitionã€‹

å®ä½“è¯†åˆ«å’ŒLinkingéƒ½è½¬åŒ–ä¸ºä¸€ä¸ªlabelingä»»åŠ¡ï¼Œè¿™é‡ŒLinkingçš„ç›®çš„æ˜¯ç»™ä¸€ä¸ªå®ä½“ç¼–å·ï¼Œä¹Ÿå°±æ˜¯åŸºäºClosed Setçš„å‡è®¾ã€‚

107.ã€ŠContextual String Embeddings for Sequence Labelingã€‹

CoNLL03çš„SOTAã€‚

å…³äºå­—å’Œè¯çš„èåˆï¼Œä¹Ÿæœ‰å¾ˆå¤šç©æ³•ï¼š

ï¼ˆ1ï¼‰ç›´æ¥èåˆåˆ†è¯ä¿¡æ¯

ï¼ˆ2ï¼‰å…ˆå»å­¦å­—çš„featureï¼Œç„¶ååŸºäºè¯åšèåˆï¼Œå°±æ˜¯è¿™ç¯‡äº†

ï¼ˆ3ï¼‰ç±»ä¼¼äºZENï¼Œèåˆn-gramçš„ä¿¡æ¯

ï¼ˆ4ï¼‰......

![img107](https://wx3.sinaimg.cn/mw690/aba7d18bly1gd0r1ep530j214a0fg41q.jpg)

è¡¥å……ä¸€ç¯‡ç›¸å…³æ–‡ç« ï¼Œã€ŠChinese NER Using Lattice LSTMã€‹,ACL2018

æƒ³æ³•ï¼šè¿™ç¯‡å·¥ä½œå’ŒZENå­˜åœ¨æŸç§ç¨‹åº¦ä¸Šçš„ç›¸å…³æ€§ï¼Œä¸è¿‡è¿™ç¯‡çš„å·¥ä½œæ˜¯åŸºäºLSTMç‰¹æ®Šçš„ç½‘ç»œç»“æ„æ¥åšçš„ã€‚è€ŒZENæ˜¯åœ¨BERTçš„ä½“ç³»ä¸‹å®Œæˆçš„ï¼Œæ¯”èµ·åˆ†ä¸åˆ†è¯ï¼Œn-gramæ˜¯ä¸€ç§æ›´åŠ çµæ´»ï¼ŒåŒæ—¶å…·æœ‰ä¿¡æ¯é‡çš„è¡¨ç¤ºã€‚

![img072](https://wx2.sinaimg.cn/mw690/aba7d18bly1gd47nc552gj20bo069t9f.jpg)

çœ‹ä¸€ä¸ªç›¸å…³çš„ï¼Œå¦‚ä½•æŠŠä¸“å®¶çŸ¥è¯†èå…¥å®ä½“è¯†åˆ«ï¼Ÿ

![ç›¸å…³](https://pic4.zhimg.com/80/v2-52eb4dc87a89861438b0c04e2e98cda3_720w.jpg)

106.ã€ŠPre-trained Models for Natural Language Processing: A Surveyã€‹

é»„è±èï¼Œé‚±é”¡é¹è€å¸ˆä»¬çš„å·¥ä½œã€‚

ç›¸å…³å·¥ä½œï¼šBERTçš„ç®€æ˜“ç‰ˆReviewï¼Œã€ŠA Primer in BERTology: What we know about how BERT worksã€‹

105.[ã€ŠA Survey on Contextual Embeddingsã€‹](https://arxiv.org/pdf/2003.07278.pdf)

104.ã€ŠConditional BERT Contextual Augmentationã€‹

åˆšåˆšçœ‹åˆ°ä¸€ç¯‡[æ–‡ç« ](https://zhuanlan.zhihu.com/p/112877845)ï¼Œå…³äºç”¨è¿‘æœŸçš„ä¸€äº›æŠ€æœ¯åšæ–‡æœ¬åˆ†ç±»å¢å¼ºçš„ã€‚å› æ­¤ï¼Œå¾ˆå¥½å¥‡å°±çœ‹äº†åŸå§‹è®ºæ–‡ï¼Œå…¶å®ç±»ä¼¼çš„æ€æƒ³ï¼Œåœ¨ç»„é‡Œå·²ç»åœ¨å¾ˆå¤šåœ°æ–¹ç”¨åˆ°äº†ï¼Œç®€å•å†™ä¸‹ç¬”è®°å§ã€‚

**æ ¸å¿ƒï¼šç”¨label embeddingæ›¿ä»£segmentation embeddingã€‚**

ä¸ºä»€ä¹ˆè¦åšæ•°æ®å¢å¼ºï¼Ÿï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆå’Œæå‡æ³›åŒ–ï¼‰

ä¸ºå•¥NLPçš„å¢å¼ºä¸å¥½æï¼Ÿuncontrollable(semantic invariance, label correctness)

è´¡çŒ®æ˜¯å•¥ï¼Ÿ

+ augment sentences without breaking the label-compatibility
+ can be applied to style transfer
+ SOTA 

ç›¸å…³å·¥ä½œï¼Ÿ

+ sample-based methods
+ generation-based methods(gan, vae)
+ domain-specific methods(åŒä¹‰è¯æ›¿æ¢ä¹‹ç±»)

103.ã€ŠFixMatch: Simplifying Semi-Supervised Learning with
Consistency and Confidenceã€‹

åˆ©ç”¨weak/strongæ ·æœ¬å¢å¼ºï¼Œå®ç°semi-supervisedçš„æ•ˆæœæå‡ã€‚è¿‘äº›å¹´æ¥ï¼ŒCVé¢†åŸŸå…³äºsemi-supervisedçš„ä¸€äº›ç›¸å¯¹ä¸é”™çš„å·¥ä½œï¼Œæ•´ä½“ä¸Šçš„ç‰¹ç‚¹å°±æ˜¯ç®€å•ï¼Œç®€å•å¹¶ä¸æ„å‘³ç€å®¹æ˜“æƒ³åˆ°ã€‚æ—¶è‡³ä»Šæ—¥ï¼Œè¶Šæ¥è¶Šè§‰å¾—ï¼Œé’ˆå¯¹DLçš„é—®é¢˜ï¼Œèƒ½å¤Ÿåœ¨â€œé“è·¯åƒä¸‡æ¡â€çš„å‰æä¸‹ï¼Œæ‰¾åˆ°æœ€åˆé€‚çš„é‚£å‡ æ¡ï¼Œåšå¯¹å‡ ä»¶äº‹ï¼ŒåŸºæœ¬å°±å¯ä»¥å¸¦æ¥å¾ˆå¤§çš„æå‡ã€‚è¿‡äºä¾èµ–è¯•é”™ï¼Œå¯¼è‡´ç»éªŒè¿™ç§ä¸œè¥¿ä¹Ÿæ˜¾å¾—å¾ˆé£˜ã€‚å¦‚æœèƒ½å¤Ÿç»™ç»éªŒä¸€ä¸ªå®šé‡æè¿°ï¼Œé‚£å°±å†å¥½ä¸è¿‡äº†ï¼Œä¸è¿‡ä¼¼ä¹ç›®å‰è¿˜æ²¡æœ‰å‘ç°ï¼ŒåŒ…æ‹¬è‡ªå·±ä¹Ÿæ˜¯ã€‚èƒ½å¤Ÿåšçš„æ˜¯ï¼Œè¿˜æ˜¯è¦ç»“åˆåœºæ™¯å’Œåˆ†æå¤§é‡çš„badcaseï¼Œé€‰æ‹©è¦åšçš„äº‹æƒ…ã€‚å¾ˆå¤šäººè„±ç¦»badcaseï¼Œç©ºè°ˆgeneralçš„ä¼˜åŒ–ï¼Œå¯¼è‡´è¯•é”™ç©ºé—´å·¨å¤§ï¼Œæˆæœ¬å·¨é«˜ï¼Œå®åœ¨å¤ªè¿‡äºç²—æ”¾ã€‚

![img103](https://wx1.sinaimg.cn/mw690/aba7d18bly1gctdqyep7oj20q50anq64.jpg)


102.ã€ŠA Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasksã€‹

æ€»ç»“äº†å¾ˆå¤šmulti-taskè®­ç»ƒçš„Trickã€‚

![img102](https://wx3.sinaimg.cn/mw690/aba7d18bly1gcsa3ifkfjj20lr0o3mzk.jpg)

101.ã€ŠSpeeding Up Neural Machine Translation Decoding by Cube Pruningã€‹

ä¸€ç§decodeç«¯çš„æŠ€æœ¯ã€‚

100.ã€ŠSentence-Level Fluency Evaluationã€‹

no-reference evaluation.

99.ã€ŠCASE: Context-Aware Semantic Expansionã€‹

æš´åŠ›çš„æ–¹æ³•ï¼Œå°†é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªå¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜ï¼ˆæ‰€æœ‰ä¸‹ä½è¯çš„é›†åˆï¼‰ã€‚ä¸ºäº†è§£å†³åˆ†ç±»æ€»æ•°è¾ƒå¤šçš„é—®é¢˜ï¼Œé‡‡ç”¨äº†ç‰¹æ®ŠTrickè§£å†³ã€‚

98.ã€ŠImproving Neural Named Entity Recognition with Gazetteersã€‹

å›´ç»•NERï¼Œæœ€è¿‘å…³æ³¨çš„å‡ ä¸ªTrickï¼š

ï¼ˆ1ï¼‰LANç”¨äºæ›¿ä»£CRF

ï¼ˆ2ï¼‰ZENï¼Œèåˆn-gramçš„ä¿¡æ¯

ï¼ˆ3ï¼‰è¿™ç¯‡æ–‡ç« ï¼Œwith Gazetteers.ï¼ˆ**TIPSï¼šã€Šç°ä»£æ±‰è¯­è¯å…¸ã€‹ä¹Ÿæ˜¯ä¸€ä¸ªé«˜åº¦ç»“æ„åŒ–çš„å¥½ä¸œè¥¿ï¼Œä¸ºå•¥æ²¡äººç”¨å‘¢ï¼Ÿ**ï¼‰

![98](https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/Improving%20Neural%20Named%20Entity%20Recognition%20with%20Gazetteers.png?raw=true)

97.å¦‚ä½•åˆ©ç”¨è¯­è¨€å­¦æå‡ä»»åŠ¡è¡¨ç°ï¼Ÿ

ä¸€ä¸ªæœ´ç´ çš„è§‚ç‚¹ï¼šæ•°æ®ä¸å¤Ÿï¼Œå…ˆéªŒæ¥å‡‘ã€‚å•¥æ˜¯å…ˆéªŒï¼Ÿæ¯”å¦‚è¯­è¨€å­¦ã€‚çŸ¥è¯†å›¾è°±ä¹Ÿå¯ä»¥è®¤ä¸ºæ˜¯å…ˆéªŒçš„ä¸€ç§è½½ä½“ï¼Œä½†æ˜¯æ›´å¤šçš„æ‰¿æ‹…çš„æ˜¯common sense/world knowledgeçš„è§’è‰².

[ã€ŠWhy BERT Fails in Commercial Environmentsã€‹](https://www.intel.ai/bert-commercial-environments/#gs.ykp1xd)

[ã€ŠAttending to Entities for Better Text Understandingã€‹](https://arxiv.org/abs/1911.04361)

[ã€ŠLinguistically-Informed Self-Attention for Semantic Role Labelingã€‹](https://arxiv.org/pdf/1804.08199.pdf)

**è¯­è¨€å­¦ä¿¡å·èåˆ**

å…³é”®é—®é¢˜ï¼š(1)å¦‚ä½•é‡åŒ–è¯­è¨€å­¦ä¿¡å·ï¼Ÿ(2)å¦‚ä½•å¯¹å¤šç§è¯­è¨€å­¦ä¿¡å·èåˆï¼Ÿ

è¯æ€§ä¿¡å·ï¼šè¯æ€§æ ‡æ³¨é›†[å‚è€ƒ](http://www.ltp-cloud.com/intro#pos_how)ï¼Œè¯­è¨€å­¦ä¿¡å·æš‚ä¸”æ¥è‡ªLTPï¼Œä¸åšåˆ†è¯ç­–ç•¥çš„å¯¹é½(LTPå’ŒWordPiece)ã€‚é‡åŒ–æ€è·¯ï¼šç›´æ¥è¡¨ç¤ºï¼›é—´æ¥è¡¨ç¤º(BERTçš„hidden representation)ï¼›

ä¾å­˜å¥æ³•ä¿¡å·ï¼šä¾å­˜å¥æ³•å…³ç³»[å‚è€ƒ](http://ltp.ai/docs/appendix.html#id5)ï¼Œé‡åŒ–æ€è·¯ï¼šç›´æ¥è¡¨ç¤ºï¼›è½¬åŒ–ä¸ºé‚»æ¥çŸ©é˜µï¼›Graph Embeddingï¼›(@å‡¡å“¥@æ¾æ—­)

è¯­ä¹‰è§’è‰²ä¿¡å·ï¼šéå¯¹é½è¾“å‡ºï¼Œåç»­è€ƒè™‘å¦‚ä½•åŠ å…¥ã€‚

åˆæ­¥ç»“è®ºï¼šæŒ‡æ ‡ä¸‹é™ã€‚çŒœæµ‹åŸå› ä¹‹ä¸€æ˜¯æ·»åŠ äº†æ–°çš„embeddingéœ€è¦å»learnï¼Œå½“å‰è®­ç»ƒé›†ä¸è¶³ä»¥learnåˆ°è´¨é‡è¾ƒé«˜çš„embeddingï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šæ„å‘³ç€åœ¨ä¿¡å·èåˆç«¯ä¼šé€ æˆnoiseçš„å¼•å…¥ã€‚

å‚è€ƒæ–‡ç« ï¼š

+ ã€ŠLinguistic Input Features Improve Neural Machine Translationã€‹

+ ã€ŠExtending Neural Question Answering with Linguistic Input Featuresã€‹

+ ã€ŠSemantics-aware BERT for Language Understandingã€‹


96.ã€ŠData Augmentation using Pre-trained Transformer Modelsã€‹

æ–‡ç« æ¯”è¾ƒäº†åŸºäºè‡ªç¼–ç (BERT)ï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(GPT-2)ï¼Œå’Œé¢„è®­ç»ƒseq2seq(BART/T5)çš„ä¸‰ç§æ¨¡å‹ç”¨äºæ•°æ®å¢å¼ºçš„æ•ˆæœã€‚å…·ä½“çš„ï¼Œæ¯”å¦‚å¯¹äºä¸€ä¸ªæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ï¼Œä¸‰ç§æ–¹å¼éƒ½å¯ä»¥åšï¼Œå“ªç§å¥½ä¸€äº›ï¼Ÿæ–‡ç« çš„ç»“è®ºæ˜¯ï¼šè€ƒè™‘åˆ°æ ‡ç­¾ä¿æŒçš„èƒ½åŠ›å’Œå¤šæ ·æ€§ï¼Œseq2seqæ•´ä½“ä¸Šè¾ƒå¥½ã€‚
åœ¨å…·ä½“æ•°æ®å¢å¼ºçš„æ–¹æ³•ä¸Šï¼Œæ–‡ç« ä¹Ÿæœ‰ä¸€äº›é˜è¿°ã€‚æ•´ä½“ä¸Šæ–‡ç« è§£å†³çš„é—®é¢˜å®ç”¨æ€§è¾ƒå¼ºï¼Œæ—¢å¯ä»¥ä½œä¸ºä¸€ç¯‡Reviewï¼Œä¹Ÿå¯ä»¥ä½œä¸ºä¸€ç¯‡æŠ€æœ¯æŠ¥å‘Šæ¥çœ‹ã€‚æ­¤å¤–ï¼Œæ–‡ä¸­çš„ä¸€äº›æ–¹æ³•è™½ç„¶æ˜¯æ”¾åœ¨æ•°æ®å¢å¼ºçš„è§’åº¦æ¥è€ƒå¯Ÿçš„ï¼Œä½†æ˜¯ç†è®ºä¸Šåº”è¯¥ä¹Ÿå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–Taskä¸Šï¼Œä¾‹å¦‚æƒ…æ„Ÿè¿ç§»ç­‰ã€‚

[ã€ŠLearning from Unlabeled Dataã€‹by Thang Luong](https://drive.google.com/file/d/1ax1-XprJHDRRv2Ru3dJwPLs3ShxcpQ3r/view)

è¿™ä¸ªTalkä¸»è¦è®²äº†ï¼šUDA+NoisyStudent(self-training)+Meenaï¼ˆchatbotï¼‰

95.ã€ŠTrain Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformersã€‹

è®¨è®ºäº†ä¸€ä¸ªæ¯”è¾ƒå®é™…çš„é—®é¢˜ï¼Œå¦‚ä¸‹ï¼š

![img95](https://bair.berkeley.edu/static/blog/compress/Flowchart.png)

94.å…³äºCensorShipçš„ä¸‰ç¯‡æ–‡ç« ï¼š

ã€ŠCreative Language Encoding under Censorshipã€‹

ã€ŠLinguistic Fingerprints of Internet Censorship: the Case of Sina Weiboã€‹

ã€ŠHow Censorship in China Allows Government Criticism but Silences Collective Expressionã€‹

93.ã€ŠA Primer in BERTology: What we know about how BERT worksã€‹

å§‘ä¸”ç®—æ˜¯ä¸€ç¯‡å…³äºBERTçš„Reviewå§ï¼Œåé¢ä¸€å®šæœ‰è´¨é‡æ›´é«˜çš„Reviewã€‚ä¸è¿‡è¿™ç¯‡æ–‡ç« çš„ç‰¹è‰²ä¸»è¦åœ¨äºå¼•ç”¨äº†æ›´å¤šå…³äºprobingçš„å·¥ä½œã€‚ç”±äºprobingå·¥ä½œï¼Œåœ¨ä»»åŠ¡è®¾è®¡çš„æ—¶å€™ï¼Œå¯èƒ½å°±å·²ç»å¼•å…¥äº†è¿‡å¤šçš„å‡è®¾ï¼Œå¯¼è‡´ç»“è®ºç†è®ºä¸Šå¸¦æœ‰ä¸€å®šçš„biasã€‚æ€»ä½“ä¸Šï¼Œä»æ—§å€¼å¾—è¯»ä¸€ä¸‹ã€‚

92.ã€ŠRethinking Bias-Variance Trade-off for Generalization of Neural Networksã€‹,é©¬æ¯…è€å¸ˆçš„å·¥ä½œ

æœŸæœ›é£é™©æ›²çº¿ = biasæ›²çº¿+varianceæ›²çº¿ï¼Œå½“biasæ›²çº¿å’Œvarianceæ›²çº¿çš„scaleä¸åŒæ—¶ï¼ŒæœŸæœ›é£é™©æ›²çº¿å°±ä¼šå‘ˆç°å‡ºä¸åŒçš„å½¢æ€ã€‚ç±»æ¯”é«˜ä¸­ç‰©ç†å­¦æ³¢è¿åŠ¨å’Œåˆ†è§£...åæ­£å¯¹é©¬è€å¸ˆè¯´ï¼Œè¿™ç¯‡æ–‡ç« å°±æ˜¯homeworkè€Œå·²ã€‚å¦‚ä¸‹ï¼š

![img92](https://wx2.sinaimg.cn/mw690/aba7d18bly1gcdmxi04loj20xl097djl.jpg)

91.ã€ŠA Generalized Framework of Sequence Generation with Application to Undirected Sequence Modelsã€‹

90.ã€ŠEnd-to-End Entity Linking and Disambiguation leveraging Word and Knowledge Graph Embeddingsã€‹

![img90](https://wx2.sinaimg.cn/mw690/aba7d18bly1gcc0x4g7fsj20k90lzq96.jpg)

89.ã€ŠSemantic Relatedness for Keyword Disambiguation: Exploiting Different Embeddingsã€‹

è¿™ç¯‡æ–‡ç« ä¸»è¦è§£å†³çš„é—®é¢˜æ˜¯ï¼š

> traditional WSD techniques do not perform well in situations in which the available linguistic information is very scarce, such as the case of keyword-based queries.

WSDä»»åŠ¡çš„å…¥é—¨ä»‹ç»ï¼šhttps://blog.csdn.net/jclian91/article/details/90117189

88.ã€ŠLabel-guided Learning for Text Classiï¬cationã€‹

æ°´æ–‡ï¼Œä¸è®®ã€‚

87.ã€ŠFast Structured Decoding for Sequence Modelsã€‹

éè‡ªå›å½’Transformerå’Œè‡ªå›å½’Transformerçš„åŒºåˆ«ï¼Œä¸»è¦çš„è´¡çŒ®åœ¨Decoderçš„æ”¹é€ ï¼Œç±»CRFï¼Œè¯¥æŠ€æœ¯ä¸ä½†å¯ä»¥ç”¨åœ¨seq2seqä¸Šï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ç”¨åœ¨å­—åˆ°å­—çš„çº é”™ä»»åŠ¡ä¸Šï¼Œå¦åˆ™åŸç”Ÿçš„CRFï¼Œä½ trainä¸ªè¯•ä¸€è¯•ï¼Ÿå­—å…¸å¤§çš„å“å±äººã€‚

![img87](https://wx4.sinaimg.cn/mw690/aba7d18bly1gc7sjgw4m3j20r60g1gpi.jpg)

86.ã€ŠJoint Embedding in Named Entity Linking on Sentence Levelã€‹

85.ã€ŠNeural Grammatical Error Correction Systems with Unsupervised Pre-training on Synthetic Dataã€‹

æ¯”è¾ƒæ–°çš„SOTAï¼Œseq2seqåšgecä»»åŠ¡ã€‚

84.ã€ŠAn Empirical Study of Incorporating Pseudo Data into Grammatical Error Correctionã€‹

å›´ç»•seq2seqçš„æ€è·¯åšgecä»»åŠ¡ï¼Œåšäº†å¾ˆå¤šå®éªŒæ€§çš„å¯¹æ¯”ï¼ŒåŒ…æ‹¬ï¼šæ•°æ®æ„é€ ç­–ç•¥ï¼Œåå¤„ç†ï¼ŒåŸå§‹æ•°æ®ä½¿ç”¨ç­‰ã€‚æœ€ç»ˆçš„ä¸€ä¸ªç»éªŒæ€§ç»“è®ºæ˜¯ï¼šGigawordä½œä¸ºç§å­æ•°æ®æº+BackTranslation+seq2seqåœ¨ç»å…¸benchmarkä¸Šå¯ä»¥å–å¾—ä¸é”™çš„ç»“æœï¼ˆçœ‹äº†ä¸‹Precisionå’ŒRecallï¼Œå‡ä¸åˆ°0.75ï¼ï¼‰ã€‚

ä¸è¿‡ï¼Œæ•´ä½“ä¸Šæä¾›äº†å¾ˆå¤šå¯ä»¥å°è¯•çš„ideaã€‚

83.ã€ŠLAMBERT: Layout-Aware language Modeling using BERT for
information extractionã€‹

æŠ½å–PDFæ—¶ï¼Œå¯ä»¥èåˆLayoutçš„ä¿¡æ¯ã€‚å°è±¡ä¸­è¾¾æ‘©é™¢åœ¨PDFæŠ½å–ä¸­ä¹Ÿæœ‰ç±»ä¼¼æƒ³æ³•çš„å·¥ä½œï¼Œ[ç›¸å…³æ¯”èµ›](https://tianchi.aliyun.com/competition/entrance/231771/information)

82.ã€ŠA New Clustering neural network for Chinese word segmentationã€‹

Clusteringçš„æ€è·¯åšåˆ†è¯ã€‚å…ˆå°†é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªmulti-labelçš„é—®é¢˜ï¼Œç„¶ååšClustering(GMM/K-means)

81.ã€ŠIdentifying Relations for Open Information Extractionã€‹ï¼Œå¼•ç”¨é‡1000+

ç»™å‡ºäº†ä¸€ç§å¼€æ”¾ä¿¡æ¯æŠ½å–çš„æ–¹å¼ï¼Œæ€»ç»“äº†æ¼æŠ¥å’Œè¯¯æŠ¥çš„patternï¼ŒåŒæ—¶ç»™å‡ºäº†ä¸€ç§åŸºäºfeatureæ„å»ºçš„ä¸‰å…ƒç»„è´¨æ£€çš„æ–¹å¼ã€‚

80.ã€ŠFine-Tuning Pretrained Language Models:
Weight Initializations, Data Orders, and Early Stoppingã€‹

éšæœºç§å­å¯¹æœ€ç»ˆç»“æœçš„å½±å“æ˜¯å¾ˆå¤§çš„ã€‚è¿™ç¯‡æ–‡ç« æ¢è®¨äº†éšæœºç§å­å¯¹æƒé‡åˆå§‹åŒ–å’Œè®­ç»ƒæ•°æ®æ’åºçš„å½±å“ã€‚

TIPSï¼šå¿ƒæƒ…ä¸å¥½ï¼Œæ¢ä¸ªç§å­ï¼Ÿ

79.ã€ŠIncorporating BERT into Neural Machine Translationã€‹,ICLR2020

ç»“è®ºï¼š7ä¸ªæ•°æ®é›†çš„SOTA

æ€è·¯ï¼šBERTåªè´Ÿè´£æŠ½featureï¼Œç„¶åå’Œseq2seqçš„encoder&decoderçš„layersçš„featureåšèåˆã€‚

ç¤ºä¾‹å›¾å¦‚ä¸‹ï¼š

 ![img79](https://wx1.sinaimg.cn/mw690/aba7d18bly1gc1i5zzevqj20ii0c5wgn.jpg)
 
 è¿™ä¸ªæ–¹å‘å¾ˆå¤šäººåœ¨æ¢ç´¢ï¼Œæ¯”å¦‚ç°åœ¨ç›¸å¯¹æœ‰æ•ˆçš„æ˜¯ï¼Œencoderç«¯ç”¨BERTåšfine-tuneï¼Œdecoderç«¯train from scratch.


78.ã€ŠHow Contextual are Contextualized Word Representations?ã€‹,EMNLP2019

å¯¹contextçš„ä¸€ç§é‡åŒ–åˆ†ææ–¹æ³•ã€‚ä¸€ä¸ªæ¦‚å¿µæ˜¯å¦æ˜¯ç„å­¦ï¼Œä¸å®šä¹‰å’Œåº¦é‡çš„æ–¹å¼æœ‰ä¸€å®šå…³ç³»ã€‚

77.ã€ŠUtilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis
and Natural Language Inferenceã€‹

![img77](https://wx4.sinaimg.cn/mw690/aba7d18bly1gbvtwwpwocj20er0dowg7.jpg)


76.ã€ŠEncode, Tag, Realize: High-Precision Text Editingã€‹,EMNLP2019

ä¸€å¤§æ—©ï¼Œæˆ‘è€å¤§å‘çš„ä¸€ç¯‡Paperã€‚

ä¸€å¼ å›¾è¯´æ˜ï¼Œå¦‚ä¸‹ï¼š

![img76](https://wx4.sinaimg.cn/mw690/aba7d18bly1gbvqpq61e9j20wd04ijsg.jpg)

é€‚åˆè§£å†³çš„é—®é¢˜ï¼šç”Ÿæˆç±»ä»»åŠ¡ä¸­ï¼Œè¾“å…¥å’Œè¾“å‡ºoverlapè¾ƒå¤šçš„æ—¶å€™ã€‚ä¼ ç»Ÿçš„seq2seqåœ¨è§£å†³è¿™ç±»é—®é¢˜æ—¶ï¼Œå°±æ˜¾å¾—æ•ˆç‡ä¼šå¾ˆä½ï¼Œä¸»è¦æ˜¯ç”±äºdecoderç«¯çš„tokenæ˜¯éƒ½è¦ç”Ÿæˆå¾—åˆ°ã€‚

ä¸»è¦å†…å®¹ï¼šEncodeç«¯æ˜¯BERT, Tagè¡¨ç¤ºé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ“ä½œ(åˆ é™¤ï¼Œä¿ç•™ï¼Œæ·»åŠ ç­‰)ï¼ŒRealizeæ˜¯é’ˆå¯¹Tagåºåˆ—çš„èšåˆé˜¶æ®µã€‚æ•´ä½“ä¸Šæ˜¯ä¸€ä¸ªåŸºäºBERTçš„åºåˆ—æ ‡æ³¨ä»»åŠ¡ï¼Œç‰¹è‰²åœ¨äºé’ˆå¯¹ç‰¹å®šä»»åŠ¡è®¾è®¡ç‰¹æ®Šçš„Tagä½“ç³»ï¼Œä»è€Œå®Œæˆä»»åŠ¡ç›®æ ‡ï¼Œå¦‚æ–‡æœ¬èåˆï¼Œå‹ç¼©ï¼Œçº é”™ç­‰ã€‚

ä¼˜ç‚¹ï¼šå…¸å‹ä»»åŠ¡ä¸Šçš„SOTA(è™½ç„¶æœ‰äº›ä»»åŠ¡æ˜¯å¾®å¼±æå‡)ï¼Œæ¨ç†é€Ÿåº¦ï¼Œæ–¹æ³•ä½“ç³»ç›¸å¯¹ç®€å•ã€‚

ç¼ºç‚¹ï¼šTagé˜¶æ®µåšä¸å¥½ï¼Œä¼šç›´æ¥å½±å“åˆ°æœ€ç»ˆæ•ˆæœã€‚å½“Tagé˜¶æ®µå¾—åˆ°çš„Vocabæ¯”è¾ƒå¤§çš„æ—¶å€™ï¼Œå¯¼è‡´æ¨¡å‹çš„åˆ†ç±»å±‚æ¯”è¾ƒé‡ï¼Œæˆ–è®¸å¯ä»¥é‡‡ç”¨åˆ†çº§åˆ†ç±»çš„æƒ³æ³•æ¥ç¼“è§£ã€‚

å…¶ä»–ï¼šåœ¨çº é”™ä»»åŠ¡ä¸Šï¼Œä½œä¸ºæ–‡ç« çš„å¯¹æ¯”å¯¹è±¡ï¼Œã€ŠNeural Grammatical Error Correction Systems with Unsupervised Pre-training on Synthetic Dataã€‹çš„Få€¼æ¯”è¯¥å·¥ä½œé«˜äº†è¿‘20ä¸ªç™¾åˆ†ç‚¹ï¼Œæ–‡ç« è®¤ä¸ºä¸»è¦çš„åŸå› æ˜¯ï¼Œå‰è€…ä½¿ç”¨äº†æ›´å¤šå¤–éƒ¨æ•°æ®ã€‚

[PRæ–‡æ¡£](https://www.jiqizhixin.com/articles/2020-02-10-8)ï¼Œæ–‡æ¡£ä¸­çš„å†…å®¹å’ŒPaperä¸å®Œå…¨ä¸€è‡´ï¼Œå¯ä»¥ç›´æ¥çœ‹Paperã€‚

75.ã€ŠA review of novelty detectionã€‹ï¼ŒMarco A.F. Pimentel 2014å¹´ï¼Œ310ç¯‡å¼•æ–‡

74.ã€ŠSnippext: Semi-supervised Opinion Mining with Augmented Dataã€‹ï¼ŒWWW2020

å±äºBERTç³»å·¥ä½œï¼ŒSnippext = MixDAï¼ˆä¸€ç§æ•°æ®å¢å¼ºæŠ€æœ¯ï¼‰+MixMatchï¼ˆä¸€ç§åŠç›‘ç£å­¦ä¹ ç®—æ³•ï¼‰ã€‚æ–‡ç« ä¸­æåˆ°ï¼Œè¿™å¥—æ–¹æ³•å·²ç»ç”¨åœ¨äº†é…’åº—è¯„è®ºèšåˆå¹³å°å’ŒæŸçŒå¤´å…¬å¸ã€‚

73.ã€ŠCan You Trust Your Modelâ€™s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shiftã€‹ï¼ŒNeurIPS2019

ç¨åä¼šå†™ä¸€ç¯‡åšå®¢æ€»ç»“è¿™ç±»é—®é¢˜ï¼Œä¸OODç›¸å…³ã€‚

![img73_1](https://wx2.sinaimg.cn/mw690/aba7d18bly1gbsukjkw9qj20z2072tbv.jpg)

![img73_2](https://wx2.sinaimg.cn/mw690/aba7d18bly1gbsukfeyquj20zc0dvtdt.jpg)

![img73_3](https://wx4.sinaimg.cn/mw690/aba7d18bly1gbsukaki2wj20z50a1adu.jpg)

è¡¥å……ï¼š

ã€ŠLearning under Concept Drift: A Reviewã€‹æ ¸å¿ƒå¤§æ¡†æ¶ï¼šæ£€æµ‹åˆ†å¸ƒçš„ä¸€è‡´æ€§ã€‚

72.ã€ŠDeep k-Nearest Neighbors: Towards Conï¬dent, Interpretable and Robust Deep Learningã€‹

> This hybrid classiï¬er combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations.

NNæ˜¯ä¸€ä¸ªéå¸¸generalçš„æ–¹æ³•ï¼Œå°è±¡ä¸­2018å¹´çš„CVPRæœ‰ç¯‡å…³äºdetectionçš„æ–‡ç« ï¼Œæ­¤å¤–ï¼Œåœ¨ä¹‹å‰åšçš„ä¸€äº›æ¯”èµ›ä¸­ï¼Œå¯ä»¥ä½œä¸ºä¸€ä¸ªpost-processingçš„æ‰‹æ®µã€‚ä¸€ç›´åœ¨æ€è€ƒçš„é—®é¢˜æ˜¯ï¼šèƒ½å¦ä½œä¸ºä¸€ç§è§£å†³é•¿å°¾åˆ†ç±»çš„æ–¹æ³•ï¼Ÿ

71.ã€ŠConcept Discovery through Information Extraction in Restaurant Domainã€‹

word2vec+cluster,ç±»ä¼¼å·¥ä½œï¼Œç±»ä¼¼æ–¹æ³•ï¼Œæ•ˆæœä¸€èˆ¬ä¸å·®ã€‚

70.ã€ŠEditable Neural Networksã€‹ï¼ŒICLR2020ï¼ŒPoster

è¿™ç¯‡æ–‡ç« æ˜¯æœ€è¿‘çœ‹åˆ°çš„éå¸¸æœ‰å¯å‘çš„æ–‡ç« ï¼Œä½“ç°åœ¨ä»¥ä¸‹å‡ ç‚¹ï¼š

ï¼ˆ1ï¼‰è¿™ç¯‡æ–‡ç« çš„æ–¹æ¡ˆå…·æœ‰å·¥ä¸šå®ç”¨æ€§ã€‚é’ˆå¯¹badcaseçš„hotfixæ‰‹æ®µã€‚

ï¼ˆ2ï¼‰é’ˆå¯¹hotfixçš„é—®é¢˜ï¼Œä¹Ÿæ­£æ˜¯lifelong learningå¯ä»¥å‘æŒ¥èƒ½åŠ›çš„åœ°æ–¹ï¼Œåªä¸è¿‡è¿™é‡Œæ˜¯few-shotçš„sampleã€‚

ï¼ˆ3ï¼‰baselineä¸­å…³äºKNNçš„éƒ¨åˆ†ä¹Ÿå¸¦æ¥äº†ä¸€äº›å¯å‘ã€‚é•¿å°¾æ•°æ®å’Œbadcaseçš„fixé—®é¢˜ã€‚

åœ¨è¿™ç¯‡æ–‡ç« çš„[OpenReview](https://openreview.net/forum?id=HJedXaEtvS)ä¸­ï¼Œå…¶ä¸­ä¸€ä¸ªRevieweræå‡ºäº†ä¸€ä¸ªæœ‰æ„æ€çš„é—®é¢˜ï¼Œè¯¥é—®é¢˜å…¶å®æ˜¯OODç›¸å…³çš„ï¼ŒåŒæ ·æ˜¯ä¸€ä¸ªæœ‰è¶£ä¸”éå¸¸é‡è¦çš„é—®é¢˜ï¼Œå¦‚ä¸‹ï¼š

![img70](https://wx4.sinaimg.cn/mw690/aba7d18bly1gbpcvlvgzhj20sz02lab0.jpg)

69.ã€ŠLifelong Learning for Sentiment Classificationã€‹

Bayesç”¨äºLLLï¼Œç»™å‡ºäº†LLçš„ä¸€ä¸ªå®šä¹‰ï¼Œéœ€è¦å››ä¸ªç»„ä»¶ã€‚

68.ã€ŠDeep learning for drugâ€“drug interaction extraction from the literature: a reviewã€‹

ä¸‹è½½ä¸åˆ°ï¼Œä½†æ˜¯æƒ³çœ‹ã€‚

67.ã€ŠThe Power of Pivoting for Exact Clique Countingã€‹ï¼ŒWSDM2020 Best Paper

ç§ä»¥ä¸ºç›¸åŒç±»å‹çš„å·¥ä½œï¼šå›´ç»•æœ€è¿‘é‚»ç›¸å…³çš„å·¥ä½œï¼Œä»¥åŠæœ¬ç¯‡æ–‡ç« ã€‚

ã€ŠA holistic lexicon-based approach to opinion miningã€‹WSDM2008ï¼ŒWSDM2020 Test of Time Award

66.ã€ŠGraph Convolution for Multimodal Information Extraction from Visually Rich Documentsã€‹ ï¼Œè¾¾æ‘©é™¢

ä¸€èˆ¬çš„åšæ³•æ˜¯ï¼šåŠç»“æ„åŒ–æ•°æ®åˆ°æ–‡æœ¬ï¼Œæ¯”å¦‚PDFè½¬åŒ–ä¸ºæ–‡æœ¬ï¼Œç„¶åå¯¹æ–‡æœ¬åšæŠ½å–ã€‚è¿™é‡Œè¯´ä¸ä»…è¦æ–‡æœ¬featureï¼ŒåŒæ—¶è¦èåˆvisual featureï¼ˆGCNè·å–ï¼‰åšsequence labelingã€‚

é—®é¢˜ï¼šæ—¢ç„¶æ˜¯åŠç»“æ„åŒ–æ•°æ®ï¼Œèƒ½å¦åªåˆ©ç”¨visual featureå‘¢ï¼Ÿ

[ç›¸å…³æ¯”èµ›](https://tianchi.aliyun.com/competition/entrance/231771/rankingList)(ä»äººç‰©PDFç®€å†ä¸­æŠ½å–äººåï¼Œå­¦å†ï¼Œå¹´é¾„ç­‰ä¿¡æ¯ï¼‰

65.ã€ŠEmpower Sequence Labeling with Task-Aware Neural Language Modelã€‹

multi-taskç”¨äºä¼˜åŒ–sequence labelingï¼ŒLM-LSTM-CRFç”¨äºsequence labelingï¼Œå›´ç»•ç›´æ¥çš„LSTM/BERTåšåºåˆ—æ ‡æ³¨ï¼Œå¯ä»¥ç”¨LM/LAN/CRFä¼˜åŒ–ã€‚

64.ã€ŠRethinking Generalization of Neural Models: A Named Entity Recognition Case Studyã€‹

ä»NERä»»åŠ¡è°ˆç¥ç»ç½‘ç»œæ¨¡å‹çš„æ³›åŒ–æ€§ã€‚å®šä¹‰äº†ä¸€äº›æ¯”è¾ƒæµ…çš„æŒ‡æ ‡ï¼Œæ‰¾å‡ºäº†å…¬æœ‰æ•°æ®é›†çš„ä¸€äº›bugã€‚çœ‹ä¼¼ç®€å•ï¼Œä½†æ˜¯è¿™äº›é—®é¢˜å®é™…åœºæ™¯ä¸‹ä¼šç»å¸¸é‡åˆ°ã€‚

63.ã€ŠA Baseline For Detecting MisClassified And Out Of Distribution Examples In Neural Networksã€‹ï¼ŒICLR2017

æå‡ºä¸€ä¸ªå¾ˆæœ‰æ„æ€çš„é—®é¢˜ï¼šé’ˆå¯¹äºŒåˆ†ç±»é—®é¢˜ï¼Œå½“ä½ çš„æ¨¡å‹é¢„æµ‹ä¸€ä¸ªæ ·æœ¬probå¾ˆä½çš„æ—¶å€™ï¼Œè¿™ä¸ªæ ·æœ¬æ˜¯è´Ÿæ ·æœ¬ï¼Œè¿˜æ˜¯è¿™ä¸ªæ ·æœ¬æ˜¯OODï¼Ÿï¼ˆout of distributionï¼‰

æœ€è¿‘åšçš„ä¸€ä¸ªåºåˆ—æ ‡æ³¨çš„å·¥ä½œä¸­è§‚å¯Ÿåˆ°ï¼šOODä¸€èˆ¬çš„probç¡®å®è¦ä½å¾ˆå¤šã€‚ä»ç½®ä¿¡åº¦çš„è§’åº¦æ¥ç†è§£ï¼Œmake senseã€‚

è¯¥æ–¹å‘ä¸Šçš„å·¥ä½œä¸é²æ£’æ€§å¼ºç›¸å…³ã€‚

62.ã€ŠQ8BERT: Quantized 8Bit BERTã€‹,NeurIPS2019

æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿçš„Topicã€‚

ç›¸å…³Paperï¼šã€ŠQuantizing deep convolutional networks for efï¬cient inference: A whitepaperã€‹

PyTorch1.3.0å¼€å§‹æ”¯æŒè¿™ä¸ªfeatureï¼Œä¸è¿‡ç›®å‰æ˜¯experimentalçš„ï¼Œ[tutorialåœ°å€](https://pytorch.org/tutorials/)

transformersçš„issuesåŒºæœ‰äººæå‡ºäº†ç›¸å…³[issue](https://github.com/huggingface/transformers/issues/2466)

61.ã€ŠPEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarizationã€‹

æç¤ºï¼šå…±52é¡µï¼Œé™„å½•40é¡µã€‚æ–¹æ³•å¦‚ä¸‹ï¼š

![img](https://wx1.sinaimg.cn/mw690/aba7d18bly1gatvxcslymj20oj0bgwgu.jpg)

60.ã€ŠAttention Is All You Needã€‹ï¼ˆTransformerçš„æ¦‚å¿µéš¾é“ä¸æ˜¯åœ¨è¿™é‡Œå®šä¹‰çš„å—ï¼Ÿä¸ºå•¥å¥½å¤špaperçå†™ï¼Œå¥½å¤šäººçè¯´ï¼Œä¸æ˜¯åªæœ‰encoderå•Šã€‚ï¼‰

äº®ç‚¹ï¼š

ï¼ˆ1ï¼‰SOTA
ï¼ˆ2ï¼‰å¹¶è¡Œ(feed forwardæ˜¯position wiseçš„ï¼›self-attentionçš„çŸ©é˜µä¹˜æ³•è®¡ç®—ï¼›multi-headç­‰)
ï¼ˆ3ï¼‰ç®€å•ï¼ˆæŠ›å¼ƒäº†recurrenceå’Œconvï¼‰

+ Attention: åœ¨å»ºæ¨¡inputå’Œoutputä¹‹é—´çš„dependencyæ—¶ï¼Œæ— éœ€è€ƒè™‘äºŒè€…ä¹‹é—´çš„distanceã€‚dependencyä¸ä¸€å®šå¿…é¡»æ˜¯distanceçš„å‡½æ•°ï¼Œå–å†³äºå¦‚ä½•å®šä¹‰dependency?

æè¿°ï¼š output = f(a query, a set of key-value pairs)

ä¸ºå•¥é€‰æ‹©dot-product attentionè€Œä¸æ˜¯additive attentionï¼Ÿ**æœ€ä¸»è¦çš„ç›®çš„è¿˜æ˜¯å·¥ç¨‹ä¸Šå¿«ã€‚å› ä¸ºå­˜åœ¨è®¡ç®—æ›´å¿«ï¼Œç©ºé—´æ›´åŠ é«˜æ•ˆçš„çŸ©é˜µä¹˜æ³•ã€‚**äºŒè€…çš„ç†è®ºæ—¶é—´å¤æ‚åº¦ä¸€æ ·ï¼Œå®é™…ä¸Šï¼Œåè€…çš„æ•ˆæœä¸ä¾èµ–äºå¾…è®¡ç®—vectorçš„ç»´åº¦å¤§å°ï¼Œä¸”æ•´ä½“ä¸Šæ•ˆæœæ›´å¥½ã€‚ä½†æ˜¯è¿˜æƒ³ç”¨å‰è€…ï¼Œé‚£å°±å¿…é¡»æ¶ˆé™¤vectorçš„ç»´åº¦å½±å“ï¼Œå› æ­¤åˆ†æ¯é™¤sqrt(d\_k)ã€‚å®é™…ä¸Šï¼Œå¯¹å‰è€…ï¼ŒVçš„weightæ˜¯ç›´æ¥è®¡ç®—å¾—åˆ°ï¼Œè€Œå¯¹åè€…ï¼Œæ˜¯learnåˆ°çš„ã€‚é‚£ä¹ˆï¼Œä¸ºå•¥d\_kä¼šå¯¹å‰è€…äº§ç”Ÿå½±å“ï¼Ÿä¸»è¦æ˜¯qkçš„varianceæ˜¯ä¸d\_kæœ‰å…³çš„ï¼Œå½“d\_kå¾ˆå¤§æ—¶ï¼Œvarianceå¾ˆå¤§ï¼Œè¿™æ ·å®¹æ˜“å¯¼è‡´gradientè¿›å…¥softmaxçš„é¥±å’ŒåŒºï¼Œç©å„¿ä¸ªé”¤å­ã€‚

Transformerçš„ä¼˜ç‚¹ï¼Ÿï¼ˆæ€è€ƒè§’åº¦ï¼‰

ï¼ˆ1ï¼‰æ¯å±‚çš„è®¡ç®—å¤æ‚åº¦

ï¼ˆ2ï¼‰å¯å¹¶è¡Œçš„æ€»é‡

ï¼ˆ3ï¼‰èƒ½å¦å¾ˆå¥½åœ°learnåˆ°long range dependencyï¼ˆå¾ˆé—æ†¾ï¼Œè¿™é‡Œdependencyä»æ—§æ˜¯distanceçš„å‡½æ•°ï¼‰

ï¼ˆ4ï¼‰å¯è§£é‡Šæ€§

+ æ—¶é—´å¤æ‚åº¦ï¼šå…³è”inputå’Œoutputä»»æ„ä¸¤ä¸ªpositonçš„ä¿¡å·ï¼Œéœ€è¦çš„æ“ä½œçš„æ¬¡æ•°ï¼ŸTransformer=O(constant),ConvS2S=O(N),ByteNet=O(logN,ä¸æ˜¯å¾ˆç¡®å®š),è¿™é‡Œçš„åŒºåˆ«å’Œç”¨æ•°ç»„è¿˜æ˜¯ç”¨é“¾è¡¨ç›¸ä¼¼ã€‚

+ Memory Networkæ˜¯åŸºäºrecurrent attentionæœºåˆ¶çš„ï¼Œä¸æ˜¯sequence-aligned recurrenceï¼ˆç±»ä¼¼2014å¹´ç»å…¸çš„seq2seq+attentionï¼‰ï¼Œä»è¿™ç‚¹å„¿æ¥è®²ï¼ŒTransformerä¹Ÿç®—æ˜¯å»¶ç»­äº†Memory Networkçš„è¡€è„‰ï¼Œå°¤è®°å¾—å½“å¹´Memory NetworkğŸ”¥è¿‡ã€‚

+ position encoding

ä¸¤ç§æ–¹å¼å¦‚ä¸‹ï¼š

ï¼ˆ1ï¼‰fixedï¼ˆç›¸å¯¹ç¼–ç ï¼‰

ï¼ˆ2ï¼‰learnable

ç»“è®ºï¼šæ•ˆæœå·®ä¸å¤šã€‚ä½†æ˜¯learnableçš„éœ€è¦é¢„å…ˆlearnåˆ°æ‰èƒ½ç”¨ï¼ˆ**BERTï¼Œé•¿åº¦512**ï¼‰ï¼Œä½†æ˜¯fixedç‰ˆå°±æ˜¯posçš„å‡½æ•°ï¼Œå¯ä»¥ç›´æ¥è®¡ç®—ï¼Œå› æ­¤å¯ä»¥æ‰©å±•åˆ°ä»»æ„inputé•¿åº¦åºåˆ—ã€‚

+ Transformerå®šä¹‰:

>  The Transformer follows encoder-decoder structure using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.

Decoderç«¯çš„ä¸‰ä¸ªç»†èŠ‚ï¼š

+ Masked Multi-Head Attention

å¯¹äºâ€œæˆ‘ çˆ± åƒ è‹¹ æœ ã€‚â€ï¼Œ é¢„æµ‹â€œåƒâ€è¿™ä¸ªtokençš„æ—¶å€™ï¼Œä¸èƒ½çœ‹åˆ°â€œè‹¹æœã€‚â€è¿™ä¸‰ä¸ªtokenã€‚

+ shifted right

è®­ç»ƒæ—¶ï¼šè¾“å…¥â€œ<sos> æˆ‘ çˆ± åƒ è‹¹ æœ ã€‚â€ï¼Œ é¢„æµ‹â€œæˆ‘ çˆ± åƒ è‹¹ æœ ã€‚<eos>â€

é¢„æµ‹æ—¶ï¼ˆè‡ªå›å½’ï¼‰ï¼šç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å‹ï¼Œä¸€æ¬¡ä¸€ä¸ªã€‚

+ å…±äº«

encoderç«¯å’Œdecoderç«¯çš„embeddingå±‚å…±äº«ï¼Œpre-softmax linearå±‚å…±äº«ã€‚è¿™é‡Œæœ‰æ„æ€çš„ç‚¹å„¿æ˜¯ï¼Œ**å°†ä¸€äº›æœºåˆ¶ç”¨äºé¢„è®­ç»ƒseq2seqæ¨¡å‹ä¸­ï¼Ÿ**

ä¸‰ç§Regularizationç­–ç•¥ï¼š

ï¼ˆ1ï¼‰æ¯ä¸ªsub-layerçš„è¾“å‡º

ï¼ˆ2ï¼‰sums of the embeddings

ï¼ˆ3ï¼‰label smoothing:è™½ç„¶ä¼šhurtåˆ°pplï¼ˆè¯¥trickçš„ç›®çš„å°±æ˜¯ä½¿å¾—æ¨¡å‹å˜å¾—unsureï¼‰ï¼Œä½†æ˜¯å¯ä»¥æé«˜bleuã€‚

å…¶ä»–Trickï¼š

ï¼ˆ1ï¼‰average last few checkpoints

ï¼ˆ2ï¼‰åŸºäºdevsetï¼Œæ‰¾åˆ°beam sizeå€¼å’Œlength penaltyå€¼

ï¼ˆ3ï¼‰maximum output length = input length + 50ï¼Œå…è®¸æ—©åœ

59.ã€ŠGPT-based Generation for Classical Chinese Poetryã€‹

æ•´ä½“æ€è·¯å¦‚ä¸‹ï¼š

![img59](https://wx1.sinaimg.cn/mw690/aba7d18bly1gana6e98duj20r30g140q.jpg)

è¯¥æ€è·¯å¯ä»¥å¯¹æ¯”ä¸­æ–‡çº é”™çš„ä¸¤ç§æ€è·¯ï¼š

ï¼ˆ1ï¼‰seq2seq

ï¼ˆ2ï¼‰åºåˆ—æ ‡æ³¨ï¼ˆè‡ªç¼–ç ï¼‰

å¯¹äºå¯¹è”ï¼Œè¯—ç­‰çš„ç”Ÿæˆï¼Œï¼ˆ1ï¼‰æ˜¯ç»å…¸æ€è·¯ï¼Œï¼ˆ2ï¼‰æ˜¯æœ¬æ–‡çš„æ€è·¯ï¼ˆè‡ªå›å½’ï¼‰ã€‚ç”¨åºåˆ—æ ‡æ³¨çš„ä¼˜ç‚¹ä¹‹ä¸€åœ¨äºï¼šå¹³è¡Œæ•°æ®æ˜¯å¤©ç„¶å¯¹é½çš„ã€‚æ­¤å¤–ï¼Œä¸¤ä¸ªé—®é¢˜ï¼š

ï¼ˆ1ï¼‰è‡ªç¼–ç å¯è¡Œï¼Ÿï¼ˆé’ˆå¯¹å¯¹é½åœºæ™¯ï¼‰

ï¼ˆ2ï¼‰ä¸­æ–‡ç»´åŸºçš„é¢„è®­ç»ƒ vs å¤è¯—è¯çš„é¢„è®­ç»ƒï¼ˆä¸¤ç§è¯­è¨€ç»“æ„ï¼‰

æœ€åï¼Œçœ‹åˆ°æ–‡ç« å†™é“ï¼š

> Though the generated poems are not perfect all the time, our preliminary experiments have shown that GPT provides a good start to promote the overall quality of generated poems.

å¿ƒå¤´ä¸€æƒŠã€‚


58.ã€ŠLooking Beyond Label Noise: Shifted Label Distribution Matters in Distantly Supervised Relation Extractionã€‹

æ²¡æœ‰è®¤çœŸè¯»ï¼Œè¿™é‡Œè—ç€ï¼Œä¸»è¦è®¨è®ºï¼šä¸€ä¸ªç»å…¸å…³ç³»æŠ½å–æ•°æ®é›†ä¸­çš„æ•°æ®biaså¯¹metricçš„å½±å“ã€‚

ç±»ä¼¼å·¥ä½œï¼š[äººè„¸è¯†åˆ«æ•°æ®é›†çš„èº«ä»½é‡åˆé—®é¢˜](https://zhuanlan.zhihu.com/p/31968327)

å…¶ä»–è¿˜æœ‰ä¸€äº›ï¼Œè®°ä¸èµ·æ¥äº†ã€‚

57.ä¸¤ä¸ªæœ‰æ„æ€çš„ç»éªŒæ€§è§‚ç‚¹ï¼š

+ ã€ŠDeep Double Descent: Where Bigger Models and More Data Hurtã€‹

> By considering larger function classes, which contain more candidate predictors compatible with the data, we are able to ï¬nd interpolating functions that have smaller norm and are thus â€œsimplerâ€. Thus increasing function class capacity improves performance of classiï¬ers.

å‚è€ƒå¦‚ä¸‹å›¾ï¼š

![57_img](https://wx1.sinaimg.cn/mw690/aba7d18bly1gaih6g6q7kj20tz0dywgw.jpg)

+ ã€ŠThe Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networksã€‹

å½©ç¥¨å‡è®¾ï¼š

> A randomly initialized, dense neural network contains a subnetwork that is initialized such that when trained in isolation, it can match the test accuracy of the original network after training for at most the same number of iterations.

å½©ç¥¨å‡è®¾çš„å¯å‘ï¼š

ï¼ˆ1ï¼‰pruning irrelevant weight

ï¼ˆ2ï¼‰retrain from scratch using only the 'lottery ticket' weights

56.ã€ŠK-BERT: Enabling Language Representation with Knowledge Graphã€‹,AAAI2020

èåˆKGçš„ä¿¡æ¯ï¼Œå•¥ä¹Ÿä¸æƒ³è¯´äº†ã€‚

55.ã€ŠPRETRAINED ENCYCLOPEDIA: WEAKLY SUPERVISED KNOWLEDGE-P RETRAINED LANGUAGE MODELã€‹,iclr2020

è¿™ç¯‡æ–‡ç« é€šè¿‡è®¾è®¡é¢„è®­ç»ƒä»»åŠ¡ï¼Œå¸Œæœ›guideæ¨¡å‹å»learnåˆ°æ›´å¤šçš„factã€‚æ•´ä½“æ€è·¯æ˜¯ï¼šå¯¹äºä¸€å¥è¯ï¼Œæ‰¾åˆ°è¯ä¸­çš„å®ä½“ï¼Œç”¨å…¶ä»–ç±»å‹ç›¸åŒï¼Œä½†æ˜¯å†…å®¹ä¸åŒçš„å®ä½“æ›¿ä»£ï¼Œç„¶åè®­ç»ƒæ¨¡å‹ï¼Œè®­ç»ƒç›®æ ‡æ˜¯å¤šä»»åŠ¡å½¢å¼ï¼Œå®ä½“æœ‰æ²¡æœ‰è¢«æ›¿æ¢+MLMã€‚è‡³äºè¿™æ ·åšçš„åŸå› ï¼Œæ–‡ç« ä¸­æåˆ°ï¼š

> Compared to the language modeling objective, entity replacement is deï¬ned at the entity level and introduces stronger negative signals. When we enforce entities to be of the same type, we pre- serve the linguistic correctness of the original sentence while the system needs to learn to perform judgment based on the factual aspect of the sentence.

ç±»ä¼¼æ€æƒ³å…¶å®å¯ä»¥ç”¨åœ¨å¾ˆå¤šå…¶ä»–åœ°æ–¹ã€‚

ç›¸å…³æ–‡ç« ï¼šã€ŠLanguage Models as Knowledge Bases?ã€‹

54.ã€ŠProbing sentence embeddings for linguistic propertiesã€‹ï¼ŒACL2018

å±äºProbingç±»å·¥ä½œï¼Œè®¨è®ºå¥å­embeddingå’Œè¯­è¨€å­¦å±æ€§ä¹‹é—´çš„å…³ç³»ã€‚è®¾è®¡çš„ä¸‰ä¸ªå±‚é¢çš„probingä»»åŠ¡å€¼å¾—å…³æ³¨ä¸€ä¸‹ï¼ˆä½†æ˜¯ä¸ªäººå¹¶ä¸è®¤ä¸ºè¿™äº›èƒ½å¤Ÿå®Œæ•´ä½“ç°è¯­è¨€å­¦å±æ€§ã€‚å¦ä¸€æ–¹é¢ï¼Œæ­¤ç±»ä»»åŠ¡çš„è®¾è®¡ä¼¼ä¹å¹¶ä¸æ˜¯ä¸€ä¸ªç®€å•çš„ä»»åŠ¡ï¼‰ï¼š

+ surface information:å¥å­é•¿åº¦ç­‰

+ syntactic information:è¯­æ³•æ ‘çš„æ·±åº¦ç­‰

+ semantic information: è¯çš„æ—¶æ€

53.ã€ŠLearning Sparse Sharing: Architectures for Multiple Tasksã€‹,Â AAAI2020

ç±»ä¼¼æ€æƒ³ä½“ç°åœ¨å¾ˆå¤šåœ°æ–¹ã€‚æ–‡ç« ä¸­çš„ä¸€å¼ å›¾æœ‰æ„æ€ï¼š

![img53](https://mmbiz.qpic.cn/mmbiz_png/vJe7ErxcLmgUZb0kT1OmH0CNKpqTd77uMiazADJHiauicTMoia6LmkjtZNDUic1GF1vnU9syIYzibKicUSej07cM60p2Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)


52.ã€ŠMulti-channel Reverse Dictionary Modelã€‹,AAAI2020

è§£å†³çš„é—®é¢˜ï¼šç»™å®šå¯¹è¯çš„æè¿°ï¼Œè¿”å›å¯¹åº”çš„è¯ã€‚

æ•´ä½“æ€è·¯ï¼šå¯¹è¯çš„æè¿°è¿›è¡Œencodeï¼Œç„¶åå’Œè¯çš„embeddingç®—è·ç¦»ã€‚å› æ­¤ï¼Œéœ€è¦å¥½çš„å¯¹è¯çš„æè¿°è¿›è¡Œç‰¹å¾æŠ½å–çš„encoderã€‚

æ–‡ç« å·¥ä½œï¼šåŠ featureã€‚ï¼ˆå¯¹äºmulti-channelç›¸å…³çš„ï¼Œå¤šæ˜¯åŠ featureï¼‰

æƒ³æ³•ï¼šä¸è°ˆnoveltyï¼Œè§£å†³çš„é—®é¢˜å¾ˆæœ‰æ„æ€ã€‚å¯¹äºæ–‡ç« å¯¹åº”çš„ç³»ç»Ÿï¼Œå¯ä»¥çˆ¬ä¸€çˆ¬ï¼Œåšweak-supervisionç›¸å…³ã€‚

51.ã€ŠLikelihood Ratios for Out-of-Distribution Detectionã€‹

ç•¥è¯»ã€‚é—®é¢˜ï¼šå¯¹OODæ ·æœ¬çš„å¤„ç†ã€‚

æƒ³æ³•ï¼šè®­ç»ƒæ¨¡å‹æ—¶ï¼Œç¬¦åˆIIDçš„å‡è®¾ï¼›ä½†æ˜¯å½“æ¨¡å‹éƒ¨ç½²åˆ°çº¿ä¸Šçš„æ—¶å€™ï¼Œæœ‰æ—¶å€™å¾ˆéš¾ä¿è¯IIDèƒ½å¤Ÿæ»¡è¶³ï¼Œé¦–å…ˆ**ä¸çŸ¥é“ä»€ä¹ˆæ—¶å€™ä¼šå‡ºç°OOD**ï¼Œæ¯”å¦‚æµ‹è¯•æ•°æ®ä¼šéšç€æ—¶é—´å˜åŒ–ã€‚**å¦‚æœå‡ºç°OODï¼Œè¿™ä¸ªæ—¶å€™å¦‚ä½•å¤„ç†OODå‘¢ï¼Ÿ**

ï¼ˆ1ï¼‰å¯¹OODèµ‹äºˆä¸€ä¸ªè¾ƒä½çš„ç½®ä¿¡åº¦ï¼Œæ¨¡å‹å¯¹å¾…OODçš„æ ·æœ¬è¦ä¿å®ˆã€‚ä½†æ˜¯ï¼Œæœ‰æ—¶å€™æ¨¡å‹ä¸ä¼šè¿™æ ·è¡¨ç°ï¼Œè¿™åˆæ˜¯å¦å¤–ä¸€ä¸ªæœ‰è¶£çš„é—®é¢˜äº†ï¼›

ï¼ˆ2ï¼‰ç»™å®šä¸€ä¸ªæ ·æœ¬ï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯OOD?è¿™ä¸ªå¯ä»¥åšäºŒåˆ†ç±»ï¼Œå¯ä»¥è®­ç»ƒä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥è½¬åŒ–ä¸ºä¸€ä¸ªoutlier detectionä»»åŠ¡ç­‰ï¼›

NeurIPS2019ï¼ŒBengioå•ç‹¬è®²äº†ä¸€èŠ‚ï¼Œä»IID->OODï¼Œå¸Œæœ›ä¹‹åçš„å·¥ä½œä»1-å‹ç³»ç»Ÿè½¬å‘2-å‹ç³»ç»Ÿï¼Œä¹Ÿå°±æ˜¯ä»æ„ŸçŸ¥å±‚è½¬å‘æ¨ç†å±‚ï¼Œæ¨ç†ä¸€å®šç¨‹åº¦ä¸Šæ˜¯å½»åº•è§£å†³OODçš„æ–¹æ³•ä¹‹ä¸€ã€‚

50.Contrastive Learning

ã€ŠRepresentation Learning with Contrastive Predictive Codingã€‹ï¼Œdeepmindï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ã€‚

ã€ŠDATA-EFFICIENT IMAGE RECOGNITION WITH CONTRASTIVE PREDICTIVE CODINGã€‹ï¼Œdeepmindï¼Œè¿™ç¯‡æ˜¯ä¸Šç¯‡ä¸»è¦åœ¨imageä¸­çš„åº”ç”¨ï¼Œåšäº†ä¸€äº›æ”¹å˜ï¼Œæ›´ååº”ç”¨ã€‚

ã€ŠMomentum Contrast for Unsupervised Visual Representation Learningã€‹ï¼Œå°†contrastive learningçš„æƒ³æ³•ç”¨äºimageé¢†åŸŸçš„æ— ç›‘ç£é¢„è®­ç»ƒã€‚

49.ã€ŠDATA-DEPENDENT GAUSSIAN PRIOR OBJECTIVE FOR LANGUAGE GENERATIONã€‹ï¼Œiclr2020

ä¸»è¦å·¥ä½œï¼šåœ¨ä¼ ç»Ÿæ–‡æœ¬ç”Ÿæˆçš„losså‡½æ•°ï¼ˆmleï¼‰ä¸­ï¼Œæ·»åŠ ä¸€ä¸ªkl termï¼Œè¿™ä¸ªtermæ˜¯åŸºäºgaussianï¼Œdata-dependentçš„ï¼Œä¸»è¦åŠ¨æœºæ˜¯è€ƒè™‘é‚£äº›émax probçš„probçš„åˆ†å¸ƒï¼Œè¿™äº›probä¹‹é—´çš„ä¿¡æ¯åŒæ ·æ˜¯éå¸¸ä¸°å¯Œçš„å“¦ã€‚ä¸¾ä¸ªç®€å•ä¾‹å­ï¼š

çŒ«ï¼ŒçŒªï¼Œè‹¹æœä¸‰åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¯¹çŒ«çš„é¢„æµ‹ç»“æœæ˜¯ï¼ˆ0.7ï¼Œ0.2ï¼Œ0.1ï¼‰ï¼Œåˆ†åˆ«å¯¹åº”çŒ«ï¼ŒçŒªï¼Œè‹¹æœï¼Œè¿™ä¸ªé¢„æµ‹æ˜¯æ­£ç¡®çš„ï¼Œä½†æ˜¯ï¼ˆ0.2ï¼Œ0.1ï¼‰ä¹Ÿæ˜¯æœ‰ä¿¡æ¯ç»“æœçš„ï¼Œè¿™ä¸ªprobå‘é‡æ„å‘³ç€çŒ«å’ŒçŒªçš„è·ç¦»æ¯”çŒ«å’Œè‹¹æœçš„è·ç¦»è¿‘å•Šã€‚

æƒ³åˆ°äº†ä»€ä¹ˆï¼ŸçŸ¥è¯†è’¸é¦å‘—ï¼ŒåŒæ ·åŒ…æ‹¬ä¸€äº›é’ˆå¯¹ç‚¹ï¼Œç”¨ä¸€ä¸ªåˆ†å¸ƒæ¥modelingçš„ä¸€äº›å·¥ä½œï¼Œå¥½åƒåœ¨ä¸€äº›regressionæ¨¡å‹ä¸­æœ‰ç”¨ã€‚æ€ä¹ˆæŒ–æ˜è¿™äº›ä¿¡æ¯ï¼Œç”¨ä¸€ä¸ªåˆ†å¸ƒæ¥è¡¨è¾¾ï¼Œè¿™é‡Œçš„é—®é¢˜æ˜¯ï¼šç”¨å•¥åˆ†å¸ƒï¼Ÿæ€ä¹ˆå¾—åˆ°åˆ†å¸ƒçš„å‚æ•°ï¼Ÿï¼ˆlearnableï¼‰

é™¤æ­¤ä¹‹å¤–ï¼Œè¯¥å·¥ä½œèåˆäº†priorçš„ä¿¡æ¯ï¼Œå®éªŒè¯æ˜ï¼Œlow resourceçš„settingä¸‹ï¼Œæå‡æ˜æ˜¾ï¼›ä½†æ˜¯æ•°æ®é‡å¤§çš„æ—¶å€™ï¼Œæ•ˆæœå˜å¼±ã€‚ä¸ç®¡æ€æ ·ï¼Œå°†priorä¿¡æ¯èåˆmodelï¼Œæ˜¯ä¸€ä¸ªæœ‰è¶£çš„æ–¹å‘ï¼Œå¯åšçš„äº‹æƒ…åº”è¯¥æŒºå¤šçš„ã€‚

æ•´ä½“ä¸Šï¼Œæ–‡ç« çš„å®éªŒï¼Œåˆ†æå’ŒåŠ¨æœºéƒ½é½å…¨ï¼Œnoveltyå€’ä¹Ÿä¸æ˜¯ç‰¹åˆ«å¤§ï¼Œä½†æ˜¯ä»æ—§æ˜¯ä¸€ç¯‡å¥½æ–‡ã€‚ä¸œå—å¤§å­¦çš„è€¿é‘«è€å¸ˆåšçš„label distribution learningå…¶å®ä¹ŸæŒºæœ‰æ„æ€ï¼Œä¸è¿‡æ²¡æœ‰æ·±æŒ–ã€‚

48.ã€ŠHOW MUCH POSITION INFORMATION DO CONVOLUTIONAL NEURAL NETWORKS ENCODE ?ã€‹ï¼Œiclr2020

ç»“è®ºï¼š**surprising degree of absolute position information that is encoded in commonly used neural networks.**

47.ã€ŠSequence Modeling with Unconstrained Generation Orderã€‹,NeurIPS 2019

seq2seqä¸­ï¼Œdecoderä¸ä¸€å®šéƒ½è¦æŒ‰ç…§left-to-rightçš„é¡ºåºï¼Œä¹Ÿå¯ä»¥ä»»æ„æ–¹å‘ã€‚ä¸»è¦çš„å·¥ä½œé›†ä¸­åœ¨ï¼š

ï¼ˆ1ï¼‰å…è®¸ä»»æ„æ–¹å‘ï¼šå»æ‰mask

ï¼ˆ2ï¼‰å¦‚ä½•å®šä¹‰æ–¹å‘ï¼Ÿç”¨position

ï¼ˆ3ï¼‰å¦‚ä½•learnï¼Ÿtoken+positionåŒæ—¶learn

æœ‰ç‚¹åç›´è§‰ï¼Ÿå¯¹å•¦ã€‚æ¥æ¥æ¥ï¼Œå…ˆå†™ç¬¬ä¸‰ä¸ªå­—ï¼Œå†å†™ç¬¬äºŒä¸ªå­—ï¼Œå†å†™ç¬¬å…«ä¸ªå­—...æ— è®ºæ€æ ·ï¼Œä»ç†è®ºä¸Šè®²ï¼Œexposure biasåº”è¯¥ä¼šç¼“è§£ã€‚å¼€ä¸ªè„‘æ´ï¼šåŸºäºbertï¼Œç”¨sequence lablelingçš„æ€è·¯æ¥åšç”Ÿæˆï¼Œè§£å†³outputç«¯å’Œinputç«¯ä¸éœ€è¦ä¸¥æ ¼å¯¹é½çš„é—®é¢˜ã€‚æ€»ä¹‹ï¼Œmemory bankä¹Ÿä¸è¦äº†å§ã€‚

46.ã€ŠMixtapeï¼šBreaking the Softmax Bottleneck Efficientlyã€‹ï¼ŒNeurIPS 2019

åŸºäºä¸€ä¸ªå‡è®¾ï¼š**Softmaxç“¶é¢ˆå°±æ˜¯è¯´æ¨¡å‹çš„ä½ç§©æ€§æ— æ³•å……åˆ†è¡¨è¾¾é«˜ç§©è¯­è¨€ç©ºé—´**ï¼Œè¡ç”Ÿå‡ºçš„ä¸¤ä¸ªæ–¹æ³•Moså’ŒMixtapeã€‚å…¶ä¸­Mosæ˜¯æŒ‡å¤šä¸ªsoftmaxçš„æ··åˆ, Mixtapeåœ¨æ•ˆæœä¸Šå’ŒMosåŸºæœ¬ä¸€è‡´ï¼Œä½†æ˜¯å¤§å¤§æå‡äº†é€Ÿåº¦ï¼Œå› ä¸ºæœ¬è´¨ä¸Šåœ¨ä¿è¯é«˜ç§©çš„åŒæ—¶ï¼Œç›¸æ¯”äºMoséœ€è¦è®¡ç®—å¤šæ¬¡softmaxï¼ŒåŒæ—¶ä¿å­˜ä¸­é—´logitï¼Œåœ¨å­˜å‚¨å’Œé€Ÿåº¦ä¸Šéƒ½æœ‰æ”¹å–„ã€‚

éœ€è¦æ€è€ƒçš„ç‚¹å„¿ï¼š

ï¼ˆ1ï¼‰å‡è®¾æœ¬èº«çš„åˆç†æ€§ï¼Ÿ

ï¼ˆ2ï¼‰â€œä¿¡æ¯å†—ä½™â€çš„æ€æƒ³åœ¨ä¸€ä¸ªä¾§é¢ä¸Šçš„å¤šç§å®ç°ï¼Ÿ

ç±»ä¼¼å·¥ä½œï¼šå„ç§å„æ ·çš„weight normalizationå®ç°......é€ƒã€‚

45.ã€Šâ€œçˆ±æƒ…åƒæ•°å­¦ä¸€æ ·å¤æ‚â€ï¼šç”¨äºç¤¾äº¤èŠå¤©æœºå™¨äººçš„æ¯”å–»ç”Ÿæˆç³»ç»Ÿã€‹

æœ€è¿‘æ¯”è¾ƒå…³æ³¨word2vecçš„æ›´å¤šçš„åº”ç”¨ï¼Œç›¸å…³å·¥ä½œåŒ…æ‹¬è¿™ç¯‡æ–‡ç« ï¼Œå¤§è¯æ—çš„å·¥ä½œï¼Œ**Transç³»åˆ—ï¼ˆè¿˜æ²¡ç³»ç»Ÿæƒ³è¿‡ï¼‰**ã€‚

è¿™ç¯‡æ–‡ç« ä¸»è¦è§£å†³ç»™å®šä¸€ä¸ªæœ¬ä½“ï¼Œç”Ÿæˆä¸€ä¸ªåŸºäºæœ¬ä½“çš„æ¯”å–»å¥ã€‚æ•´ä½“åˆ†ä¸ºä¸‰ä¸ªå·¥ä½œï¼š

ï¼ˆ1ï¼‰æ‰¾å–»ä½“

ï¼ˆ2ï¼‰æ‰¾è¿æ¥è¯

ï¼ˆ3ï¼‰å¥—æ¨¡ç‰ˆ

è¿™ä»¶äº‹æƒ…èƒ½åšçš„æœ¬è´¨åŸå› ï¼šæœ¬ä½“å¾€å¾€æ¥è‡ªæŠ½è±¡åŸŸï¼Œè€Œå–»ä½“å¾€å¾€æ¥è‡ªå…·ä½“åŸŸã€‚æ¢è¨€ä¹‹ï¼Œäººä»¬è®¾æ³•åˆ©ç”¨æ¯”å–»ï¼Œç”¨å®¹æ˜“ç†è§£çš„å…·ä½“æ¦‚å¿µï¼ˆå³å–»ä½“ï¼‰æ¥è§£é‡Šå’Œè¡¨è¾¾ä¸æ˜“ç†è§£çš„æŠ½è±¡æ¦‚å¿µï¼ˆå³æœ¬ä½“ï¼‰ã€‚ï¼ˆé€šä¿—ç‚¹è¯´ï¼šä½ å’‹è¯´éƒ½æ˜¯å¯¹çš„ã€‚å…¶å®ï¼Œå¤è¯—è¯ä¹Ÿæ˜¯è¿™æ ·æ»´ï¼Œå¤è¯—è¯çš„éŸµå‘³åœ¨äºè¯—è¯æœ¬èº«å’Œè¯»è€…ä¹‹é—´çš„äº¤äº’ï¼Œå…·ä½“æ¯”å¦‚é€šè¿‡è”renwuæƒ³çš„æ–¹å¼ç­‰ï¼‰
33.ã€ŠImage Classiï¬cation with Deep Learning in the Presence of Noisy Labels: A Surveyã€‹

44.ã€ŠNEZHA: Neural Contextualized Representation for Chinese Language Understandingã€‹,[ç›¸å…³ä»‹ç»](https://mp.weixin.qq.com/s/RkCLSRyy_GuLOXMVSzTMdA)

æ”¹è¿›ç»´åº¦ï¼š

æ¨¡å‹ï¼šç›¸å¯¹ä½ç½®ç¼–ç çš„åˆ©ç”¨ï¼ˆFunctional Relative Positional Encodingï¼Œä¸éœ€è¦learnã€‚ï¼‰

é¢„è®­ç»ƒä»»åŠ¡ï¼šè¯çº§Mask+Spanä¿¡æ¯çš„åˆ©ç”¨

è®­ç»ƒç®—æ³•ï¼šæ··åˆç²¾åº¦+é€‚ç”¨äºå¤§Batch(300000)çš„ä¼˜åŒ–å™¨LAMB

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆMixed Precision Trainingï¼‰æ–¹å¼ï¼Œåœ¨ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰çš„å˜é‡åŒ…æ‹¬weightï¼Œactivationå’Œgradientéƒ½æ˜¯ç”¨FP32ï¼ˆå•ç²¾åº¦æµ®ç‚¹æ•°ï¼‰æ¥è¡¨ç¤ºã€‚è€Œåœ¨æ··åˆç²¾åº¦è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸€ä¸ªstepä¼šä¸ºæ¨¡å‹çš„æ‰€æœ‰weightç»´æŠ¤ä¸€ä¸ªFP32çš„copyï¼Œç§°ä¸ºMaster  Weightsï¼Œåœ¨åšå‰å‘å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼ŒMaster Weightsä¼šè½¬æ¢æˆFP16ï¼ˆåŠç²¾åº¦æµ®ç‚¹æ•°ï¼‰æ ¼å¼ï¼Œæƒé‡ï¼Œæ¿€æ´»å‡½æ•°å’Œæ¢¯åº¦éƒ½æ˜¯ç”¨FP16è¿›è¡Œè¡¨ç¤ºï¼Œæœ€åæ¢¯åº¦ä¼šè½¬æ¢æˆFP32æ ¼å¼å»æ›´æ–°Master Weightsã€‚

Ablation Study: ä½ç½®ç¼–ç ï¼Œmaskingç­–ç•¥ï¼Œspané¢„æµ‹ä»»åŠ¡ï¼Œè®­ç»ƒåºåˆ—çš„é•¿åº¦ï¼Œè®­ç»ƒè¯­æ–™çš„å¤§å°å‡èƒ½å¸¦æ¥æå‡ï¼Œå…¶ä¸­ä½ç½®ç¼–ç ä¼šå¸¦æ¥æ˜¾è‘—æå‡ï¼›

**æœ€å¤§çš„æ”¶è·ï¼šä½ç½®ç¼–ç çš„æ”¹è¿›ã€‚**

43.ã€ŠDEEP ENSEMBLES:A LOSS LANDSCAPE PERSPECTIVEã€‹

**Why do ensembles trained with just random initialization work so well in practice?**

42.ã€ŠLearning From Positive and Unlabeled Data: A Surveyã€‹

ç›¸å…³ï¼šã€ŠA Survey on Postive and Unlabelled Learningã€‹

41.ã€ŠDistantly Supervised Named Entity Recognition using Positive-Unlabeled Learningã€‹

ç»„é‡Œå¤§ä½¬ä»Šå¤©åˆ†äº«äº†è¿™ç¯‡æ–‡ç« ã€‚ä¸»è¦è®¨è®ºçš„é—®é¢˜æ˜¯å¦‚ä½•åˆ©ç”¨è¯å…¸åšNERï¼Ÿå€ŸåŠ©äºPU Learningçš„æ€è·¯ï¼Œå…³äºPU Learningçš„æƒ³æ³•è®°å¾—åœ¨æŸä¸ªä½ç½®å†™è¿‡ã€‚

åŸºæœ¬æ­¥éª¤ï¼š

ï¼ˆ1ï¼‰ç¬¬ä¸€æ­¥ï¼šæŒ‰ç…§å­—å…¸å»åŸå§‹æ•°æ®ä¸­åšæ ‡æ³¨ã€‚æ ‡0æˆ–è€…1ï¼›

ï¼ˆ2ï¼‰ç¬¬äºŒæ­¥ï¼šé’ˆå¯¹æ¯ä¸ªå®ä½“ç±»åˆ«ï¼Œè®­ç»ƒä¸€ä¸ªäºŒåˆ†ç±»å™¨ï¼›

ï¼ˆ3ï¼‰ç¬¬ä¸‰æ­¥ï¼šèšåˆæ ‡æ³¨ç±»åˆ«ä¸ªæ•°çš„äºŒåˆ†ç±»å™¨çš„é¢„æµ‹ç»“æœï¼›

ï¼ˆ4ï¼‰å…¶ä»–ï¼šå¯ä»¥é€æ­¥å®Œå–„å­—å…¸ï¼Œæå‡æ•ˆæœï¼›

æŸå¤±å‡½æ•°ä¸æ˜¯ce lossï¼Œè€Œæ˜¯ä¸€ä¸ªæ±‚å·®å‡½æ•°ï¼ˆç±»ä¼¼äºmarginï¼‰ã€‚

æƒ³æ³•ï¼šå¦‚ä½•ç”¨åœ¨ä¸­æ–‡åœºæ™¯ï¼Ÿ

40.ã€Š150 Successful Machine Learning Models: 6 Lessons Learned at Booking.comã€‹,KDD2019, Applied Data Science Track

ä»Booking.comå‘å¸ƒçš„150ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ€»ç»“äº†6æ¡ç»éªŒï¼Œå¹¶æä¾›äº†ä¸€äº›caseåˆ†æã€‚æ¯”å¦‚ï¼šæé«˜æ¨¡å‹çš„æ€§èƒ½å¹¶ä¸ä¸€å®šä¼šè½¬åŒ–ä¸ºä¸šåŠ¡ä»·å€¼çš„å¢é•¿ï¼›ä¸€äº›æœ€å¼ºå¤§çš„æ”¹è¿›å¹¶éæ˜¯åœ¨ç»™å®šsettingçš„ä¸Šä¸‹æ–‡ä¸­æ”¹è¿›æ¨¡å‹ï¼Œè€Œæ˜¯æ›´æ”¹settingæœ¬èº«ï¼›å‡å°‘é¢„æµ‹æœåŠ¡çš„å»¶è¿Ÿï¼ˆæ¨¡å‹å†—ä½™/ç¨€ç–æ¨¡å‹/é¢„è®¡ç®—å’Œç¼“å­˜/è¯·æ±‚æ‰“åŒ…/å°½å¯èƒ½å°‘çš„ç‰¹å¾å˜æ¢ï¼‰ï¼Œå°½æ—©è·å¾—å…³äºæ¨¡å‹è´¨é‡çš„åé¦ˆï¼ˆå“åº”åˆ†å¸ƒåˆ†æï¼‰ç­‰

ç›¸å…³æ–‡ç« ï¼š

ï¼ˆ1ï¼‰[ã€ŠMachine Learning:The High-Interest Credit Card of Technical Debtã€‹](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43146.pdf)

ï¼ˆ2ï¼‰[ã€ŠHidden Technical Debt in Machine Learning Systemsã€‹](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf)

ï¼ˆ3ï¼‰[Github: Production Level Deep Learning](https://github.com/alirezadir/Production-Level-Deep-Learning)

39.ã€ŠOpen Domain Web Keyphrase Extraction Beyond Language Modelingã€‹ï¼ŒEMNLP2019

å¼€æ”¾åŸŸçš„å…³é”®çŸ­è¯­æŠ½å–ã€‚ä¸»è¦è´¡çŒ®ï¼š

ï¼ˆ1ï¼‰åšäº†ä¸€ä¸ªç›®å‰å¯èƒ½æœ€å¤§çš„å¼€æ”¾åŸŸçš„è‹±æ–‡çŸ­è¯­æŠ½å–æ•°æ®é›†ã€‚

ï¼ˆ2ï¼‰**æå‡ºåŸºäºç½‘é¡µæœç´¢çš„å¼±ç›‘ç£æ–¹æ¡ˆã€‚**ï¼ˆç›®å‰çš„ç½‘é¡µæœç´¢ï¼Œæ˜¯åŸºäºçº¯å­—ç¬¦ä¸²åŒ¹é…ï¼Œä½œä¸ºå€™é€‰é¡µé¢ï¼Œå¾ˆæ—©è‡ªå·±å°±æœ‰è¿™ä¸ªæƒ³æ³•ï¼Œå…¶å®æƒ³åŠæ³•ç”¨èµ·æ¥è¿˜æ˜¯å¾ˆé…·çš„ã€‚ï¼‰

ï¼ˆ3ï¼‰æå‡ºåˆ©ç”¨visual featureåšå¢å¼ºï¼Œå¯èƒ½å’Œæ•°æ®é›†æ„å»ºæœ‰å…³å§ï¼Œä¸ªäººä¸æ˜¯å¾ˆæ„Ÿå…´è¶£ã€‚

ï¼ˆ4ï¼‰åšäº†ä¸€ä¸ªæœ‰ç‚¹ä¸‘é™‹çš„æ¶æ„ï¼ˆåŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼‰ã€‚

éš¾ç‚¹ï¼šå½“è¦å»learnä¸€ä¸ªéå¸¸generalçš„æ¦‚å¿µçš„æ—¶å€™ï¼Œå¯èƒ½æ˜¯hard to learnçš„ã€‚

38.ã€ŠGenerating Abstractive Summaries with Finetuned Language Modelsã€‹

Alexander M. Rushç»„çš„å·¥ä½œï¼Œè®¨è®ºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ç”¨äºæ–‡æœ¬ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åœ¨seq2seqæ¶æ„ä¸‹ã€‚ç›®å‰æœ‰å¾ˆå¤šäººåœ¨å…³æ³¨è¿™ä¸ªæ–¹å‘ï¼ŒåŒ…æ‹¬è‡ªå·±ç»„é‡Œä¹Ÿåœ¨åšä¸€äº›æ¢ç´¢ï¼Œä¸è¿‡æ•ˆæœä¸æ˜¯ç‰¹åˆ«å¥½ã€‚è¿™ç¯‡æ–‡ç« ï¼Œå¤§è‡´æ˜¯decoderç«¯ç”¨äº†ä¸€ä¸ªé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä¸è¿‡å–å¾—çš„ç»“æœ<=transformer+copyåœ¨æ‘˜è¦ä»»åŠ¡ä¸Šçš„æ•ˆæœã€‚

37.ã€ŠTENER: Adapting Transformer Encoder for Named Entity Recognitionã€‹

é­”æ”¹æ¨¡å‹ä¸Šæœ‰å¯å‘ã€‚æ–‡ç« ä¸å’ŒBERTæ¯”ï¼Œå…¶å®æ„Ÿè§‰å¯¹æ¯”æ„ä¹‰ä¹Ÿä¸æ˜¯å¾ˆå¤§ã€‚å¦‚æœåŸºäºæ­¤ç»“æ„ï¼Œåšpretrainï¼Œä¼šä¸ä¼šè¿›ä¸€æ­¥æå‡BERTåœ¨NERä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Ÿè™½ç„¶æ–‡ç« è®²äº†Transformeråœ¨NERä»»åŠ¡ä¸Šè¡¨ç°ä¸å¥½çš„åŸå› ï¼Œä½†æ˜¯å¾ˆå¥½å¥‡æ˜¯å¦æ˜¯çœŸçš„ä¸å¥½ï¼Ÿï¼ˆè¿™ä¸ªéœ€è¦æ›´å¤šå®éªŒç»“æœçš„æ”¯æŒã€‚åœ¨ä¸€ä¸ªå–„äºè®²æ•…äº‹çš„å¹´ä»£ï¼Œæ€»å¾—å‘ç”Ÿç‚¹ä»€ä¹ˆæ‰æ˜¾å¾—å¯ä¿¡ã€‚ï¼‰

36.ã€ŠA Deep Look into Neural Ranking Models for Information Retrievalã€‹

ä¸€ç¯‡å…³äºRankingçš„ç»¼è¿°æ–‡ç« ã€‚hinge losså¯ä»¥ç”¨åœ¨pairwiseçš„lträ»»åŠ¡ä¸­ã€‚

35.ã€ŠPyTorch: An Imperative Style, High-Performance Deep Learning Libraryã€‹ï¼ŒNIPS2019

è®¨è®ºç³»ç»Ÿå“²å­¦çš„å­¦æœ¯æ–‡ç« ï¼Œå…¶å®å®˜æ–¹åšå®¢å·²ç»è®¨è®ºäº†å¾ˆå¤šç›¸å…³æ€æƒ³äº†ï¼Œæ‰€ä»¥è¿™ç¯‡ä½œä¸ºå­¦æœ¯æ–‡ç« å¯èƒ½å°±æ˜¯æ–¹ä¾¿å¤§å®¶å¼•ç”¨å§ã€‚ä½œä¸ºå®‡å®™æœ€å±Œæ¡†æ¶ï¼ŒPyTorchå·²ç»ä¸éœ€è¦ç”¨ä¸€ç¯‡æ–‡ç« çš„å¼•ç”¨é‡æ¥è¯æ˜è‡ªå·±äº†ã€‚

34.ã€ŠUsing Knowledge Graphs for Fact-Aware Language Modelingã€‹ï¼ŒACL2019

ç›¸å…³çš„æƒ³æ³•ï¼š

ï¼ˆ1ï¼‰bertç³»é€šè¿‡mlmèƒ½å¦ç›´æ¥fact-awareï¼Ÿå·²ç»æœ‰äººåšäº†ä¸€äº›æ¢ç´¢ã€‚

ï¼ˆ2ï¼‰æ²¡æœ‰æ²¡å…³ç³»ã€‚æŠŠkgçš„ä¿¡æ¯èå…¥encoderç«¯ï¼Œæå‡decoderçš„èƒ½åŠ›ï¼Œå°±æ˜¯è¿™ç¯‡äº†ã€‚

ï¼ˆ3ï¼‰ç»“åˆå„ä¸ªä»»åŠ¡ï¼Œèåˆkgçš„ä¿¡æ¯ï¼Œé‚£ä¹ˆæ€ä¹ˆèåˆkgçš„ä¿¡æ¯ï¼Œå°±æ˜¯ä¸€ä¸ªå¯ä»¥ç©å„¿å‡ºèŠ±å„¿æ¥çš„äº‹æƒ…ã€‚

33.ã€ŠSpam Review Detection with Graph Convolutional Networksã€‹ï¼ŒCIKM2019æœ€ä½³åº”ç”¨è®ºæ–‡

å°†GCNç”¨äºé—²é±¼çš„åƒåœ¾è¥é”€å¹¿å‘Šæ£€æµ‹ã€‚ç‰¹å¾æå–å±‚ä¸Šåˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œåˆ†åˆ«æ˜¯åŸºäºå•†å“-ç”¨æˆ·-è¯„è®ºçš„äºŒéƒ¨å›¾å’Œä¸åŒè¯„è®ºæ„æˆçš„å›¾ï¼ŒGCNä½œä¸ºç‰¹å¾æå–å™¨ã€‚åˆ†ç±»å±‚é‡‡ç”¨TextCNNã€‚

32.ã€ŠChinese Word Segmentation as Character Taggingã€‹ï¼Œ2003å¹´çš„æ–‡ç« ï¼Œå¼•ç”¨é‡å¾ˆé«˜ã€‚

å¥½åƒæ˜¯ç¬¬ä¸€æ¬¡å°†åˆ†è¯é—®é¢˜modelingä¸ºä¸€ä¸ªtaggingé—®é¢˜æ¥åšï¼Œå…±æœ‰å››ä¸ªæ ‡ç­¾ï¼ˆLL/MM/RR/LRï¼‰å¦‚ä¸‹ï¼š

![img_word_segmentation](https://wx3.sinaimg.cn/mw690/aba7d18bly1g9cvr16rhwj20t808t40d.jpg)

31.ã€ŠHow to Ask Better Questions? A Large-Scale Multi-Domain Dataset for Rewriting Ill-Formed Questionsã€‹

åšäº†ä¸€ä¸ªé—®é¢˜æ”¹å†™çš„æ•°æ®é›†ã€‚

30.ã€ŠLearning Semantic Hierarchies via Word Embeddingsã€‹ï¼ŒACL2014

ã€Šå¤§è¯æ—ã€‹ï¼Œç”¨word2vecçš„æ€è·¯åšâ€œä¸Šä¸‹ä½â€å…³ç³»æŒ–æ˜ã€‚word2vecç”¨å¥½äº†ï¼Œåº”è¯¥å¯ä»¥è§£å†³å¾ˆå¤šé—®é¢˜ï¼ŒåŒ…æ‹¬embeddingï¼Œç›¸å…³æ€§ç­‰ä»»åŠ¡ã€‚

ç”¨äºå¼€æ”¾åŸŸçš„å®ä½“ç±»åˆ«å±‚æ¬¡åŒ–ï¼ˆä¸Šä¸‹ä½å…³ç³»ï¼‰ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ç”¨è¯å‘é‡ä¹‹é—´çš„å·®å€¼åˆ»ç”»ä¸Šä¸‹ä½å…³ç³»ã€‚å·¥ä½œåˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šè¯å‘é‡å­¦ä¹ ï¼Œæ˜ å°„çŸ©é˜µå­¦ä¹ å’Œä¸Šä¸‹ä½å…³ç³»åˆ¤æ–­ã€‚ä¸Šä¸‹ä½å…³ç³»çš„åˆ¤æ–­ä¹Ÿå¯å½’äºå…³ç³»æŠ½å–ä»»åŠ¡ï¼Œæ˜¯ä¸€ç§ç‰¹æ®Šçš„è¾ƒä¸ºæŠ½è±¡çš„å…³ç³»ã€‚å› æ­¤å¯ä»¥å¾ˆè‡ªç„¶åœ°å°†æ–‡ç« çš„æƒ³æ³•æ‹“å±•åˆ°å…³ç³»åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œå¯¹æ¯ç±»å…³ç³»å­¦ä¹ ä¸€ä¸ªæ˜ å°„çŸ©é˜µã€‚å½“ç”¨äºSPOä¸‰å…ƒç»„ç›´æ¥æŠ½å–æ—¶ï¼Œéœ€è¦æœ‰é’ˆå¯¹æ€§çš„æ˜ å°„å…³ç³»å­¦ä¹ æ–¹æ³•ã€‚

29.ã€ŠStatistical Machine Translation: IBM Models 1 and 2ã€‹ï¼ŒMichael Collins

Noisy Channel Modelçš„ç»å…¸æ¡ˆä¾‹ï¼šä½œä¸ºç»Ÿè®¡æ¨¡å‹ç”¨äºæœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå¯ä»¥ç”¨äºæ‹¼å†™çº é”™ï¼ŒAuto Suggestionç­‰ï¼Œå¤§äºŒæ—¶å®ç°çš„Bayes Mattingä¹Ÿæœ‰ç±»ä¼¼çš„æ„Ÿè§‰ã€‚æ€»ä¹‹ï¼ŒNCMæ˜¯ä¸€ä¸ªæ¯”è¾ƒgeneralçš„ç†è®ºæ¨¡å‹ã€‚

28.ã€ŠFew-Shot Sequence Labeling with Label Dependency Transfer and Pair-wise Embeddingã€‹

 Few-Shot Learningçš„å·¥ä½œï¼Œç”¨äºå‘½ä»¤å®ä½“è¯†åˆ«ä»»åŠ¡ä¸Šã€‚

27.ã€ŠHierarchically-Reï¬ned Label Attention Network for Sequence Labelingã€‹,EMNLP2019

Label Embedding+Attentionç”¨äºsequence labelingã€‚å¹³å‡æå‡äº†ä¸è¶³ä¸€ä¸ªç™¾åˆ†ç‚¹ï¼Œå¯èƒ½å’Œç›¸å…³ä»»åŠ¡åŸæ¥çš„æŒ‡æ ‡å·²ç»è¾ƒé«˜æœ‰å…³ç³»ã€‚ä¸è¿‡å·¥ä½œåšçš„è¿˜æ˜¯æ¯”è¾ƒå¹²å‡€çš„ã€‚

æ–‡ç« çš„ä¸»è¦è´¡çŒ®ï¼šç²¾åº¦ç•¥æœ‰æå‡çš„å‰æä¸‹ï¼Œé€Ÿåº¦æå‡äº†ä¸å°‘ã€‚ä¸»è¦çš„æ–¹å¼æ˜¯åŸºäºlabel probability distributionï¼Œåšself-attentionã€‚ä»æ•ˆæœæ¥çœ‹ï¼Œself-attentionèƒ½å¤Ÿæ›¿ä»£crfçš„æ•ˆæœï¼ŒåŒæ—¶ç”±äºmulti-head self-attentionçš„å¹¶è¡Œç‰¹æ€§ï¼Œå› æ­¤ï¼Œé€Ÿåº¦ä¸Šå»äº†ã€‚ç»å…¸åºåˆ—æ ‡æ³¨çš„æ¨¡å‹ç»“æ„æ˜¯bilstm+crfï¼Œè¿™ç¯‡è¯´ï¼Œbilstm+lanä¹Ÿé˜”ä»¥ã€‚è¿‘ä¸€æ®µçš„å·¥ä½œè¯æ˜ï¼Œbertå¤šæ•°æƒ…å†µä¸‹éƒ½ä¸éœ€è¦crfï¼Œè¿™ç¯‡æ–‡ç« ä¹Ÿç®—æ˜¯æä¾›äº†ä¸€ä¸ªä½è¯ï¼Œæ—¢ç„¶crféƒ½å¯ä»¥è¢«self-attentionæ›¿ä»£ï¼Œé‚£ç›´æ¥æŠŠencoderä¹Ÿç”¨self-attentionæ›¿ä»£å®Œäº‹å„¿äº†ï¼Œç»“æ„è¿˜æ˜¾å¾—æ›´åŠ æ¸…æ™°ã€‚å†ä¸€æ¬¡è¯æ˜ï¼šã€Šattention is all you needã€‹ï¼Œé€ƒã€‚


26.ã€ŠMulti-instance Multi-label Learning for Relation Extractionã€‹

2012å¹´çš„å·¥ä½œï¼Œå¼•ç”¨é‡400+ã€‚å°†distant-supervisionå¾—åˆ°çš„æ•°æ®ï¼Œå»ºæ¨¡ä¸ºä¸€ä¸ªmimlé—®é¢˜ã€‚ä¸»è¦æŠ€æœ¯ï¼šgraphical model+EMã€‚æ–‡ç« å¼•ç”¨äº†å‘¨å¿—åè€å¸ˆçš„mimlçš„ç›¸å…³å·¥ä½œã€‚

å»¶ä¼¸æ€è€ƒï¼š

ï¼ˆ1ï¼‰snorkelçš„ç›¸å…³æŠ€æœ¯å¯ä»¥ç»“åˆdistant-supervisionï¼Œæ¯”å¦‚denoisingã€‚

ï¼ˆ2ï¼‰æ›´åŠ generalçš„relation extractionå¯ä»¥å…³æ³¨fewrelçš„è¿›å±•ã€‚

ï¼ˆ3ï¼‰distant-supervisionéœ€è¦å…³æ³¨multi-instanceçš„ç›¸å…³å·¥ä½œã€‚

ç›¸å…³å·¥ä½œï¼šã€ŠRelation Extraction with Multi-instance Multi-label Convolutional Neural Networksã€‹

25.ã€ŠDistant supervision for relation extraction without labeled dataã€‹

2009å¹´çš„å·¥ä½œï¼Œå¼•ç”¨éå¸¸å¤šï¼Œä½†æ˜¯å¹¶édistant-supervisionç¬¬ä¸€æ¬¡æå‡ºï¼Œä½œè€…ä¹‹ä¸€æ˜¯Dan Jurafskyã€‚æ–‡ç« åœ¨å…³ç³»æŠ½å–çš„æ—¶å€™ï¼Œè¿˜æ˜¯åŸºäºäººå·¥æ„å»ºçš„featureï¼ŒåŒ…æ‹¬syntacticå’Œlexicial featureç­‰ã€‚

ç›¸å…³å¯å‘ï¼š

ï¼ˆ1ï¼‰regular expræ˜¯learnableçš„ï¼›instanceå’Œpatternçš„snowballå¼ç©æ³•ã€‚

ï¼ˆ2ï¼‰distant-supervisionä¸‹ï¼Œmulti-instanceçš„åˆ©ç”¨æ˜¯äº®ç‚¹ã€‚

24.ã€ŠKG-BERT: BERT for Knowledge Graph Completionã€‹

è¿™ç¯‡å·¥ä½œé‡‡ç”¨äº†å’Œä¹‹å‰äº‹ä»¶åˆ¤åˆ«æ¨¡å‹ç±»ä¼¼çš„æ€è·¯ï¼Œä¸åŒä¹‹å¤„åœ¨äºç›´æ¥åŸºäºä¸‰å…ƒç»„åšï¼Œä¸åŒ…å«ä¸Šä¸‹æ–‡çš„å…·ä½“æè¿°ï¼Œåœ¨ç›¸å…³æ•°æ®é›†ä¸Šå–å¾—äº†SOTAç»“æœã€‚è™½ç„¶ä½œè€…ç»™å‡ºäº†ä¸€äº›è§£é‡Šï¼Œä¸è¿‡æ–‡ç« å®éªŒåšçš„ä¸å¤Ÿå……è¶³ï¼Œå¹¶ä¸”è§£é‡Šä¼¼ä¹ä¸æ˜¯ç‰¹åˆ«å…·æœ‰è¯´æœæ€§ã€‚

ç»“è®ºï¼šçŒæ°´ã€‚

23.**å¦‚ä½•ä½¿ç”¨æ›´å¤šçš„å¤–éƒ¨æ•°æ®æå‡æ¨¡å‹æ•ˆæœï¼Ÿ**

ï¼ˆ1ï¼‰ã€ŠExploiting Monolingual Data at Scale for Neural Machine Translationã€‹,EMNLP2019

è§‚ç‚¹ï¼šåœ¨æœºç¿»ä¸­ï¼Œå•ç‹¬åœ°åˆ©ç”¨srcç«¯åšForward Translationå’Œå•ç‹¬åœ°åˆ©ç”¨tgtç«¯åšBack Translationï¼Œåœ¨æ¨¡å‹æ€§èƒ½æå‡åˆ°ä¸€å®šé˜¶æ®µåéƒ½ä¼šæœ‰æŸå¤±ï¼Œä¸è¿‡äºŒè€…çš„æŸå¤±é€Ÿåº¦ä¸åŒã€‚é‚£ä¹ˆï¼Œè¿™ç¯‡æ–‡ç« ç»¼åˆåˆ©ç”¨äºŒè€…æå‡æœºç¿»çš„æ•ˆæœã€‚

æ•´ä½“ä¸Šæ•°æ®åˆ©ç”¨æµç¨‹ï¼šæ ‡æ³¨çš„å¹³è¡Œè¯­æ–™->æ··åˆè¯­æ–™ï¼ˆæ ‡æ³¨+ä¼ªæ ‡ç­¾+**åŠ å™ª**ï¼‰->ä¼ªæ ‡ç­¾è¯­æ–™ã€‚

è¿‡ç¨‹ä¸­éœ€è¦è®­ç»ƒ/å¾®è°ƒçš„ç¿»è¯‘æ¨¡å‹ä¸ªæ•°ï¼š8ä¸ªã€‚

éœ€è¦å¼ºè°ƒçš„æ˜¯ï¼šåŠ å™ªå¾ˆé‡è¦ã€‚

ï¼ˆ2ï¼‰ã€ŠSelf-training with Noisy Student improves ImageNet classificationã€‹ï¼Œä¸¤å¤©å‰çš„æ–‡ç« 

è´¡çŒ®ï¼šæ ‡å‡†çš„self-trainingçš„èŒƒå¼ï¼Œå°†ImageNetçš„åˆ†ç±»æŒ‡æ ‡æé™åˆæ¨è¿›äº†ä¸€ç‚¹å„¿ã€‚

éœ€è¦å¼ºè°ƒçš„æ˜¯ï¼šstudentçš„å­¦ä¹ éœ€è¦åŠ å™ªã€‚åŸæ–‡å¦‚ä¸‹ï¼š

During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as good as possible. But during the learning of the student, we inject noise such as data augmentation, dropout, stochastic depth to the stu- dent so that the noised student is forced to learn harder from the pseudo labels. 

ä¸Šè¿°ä¸¤ç¯‡æ–‡ç« çš„å»¶ä¼¸æ€è€ƒï¼š

  å½“æ•°æ®æ˜¯æ„é€ å¾—åˆ°çš„æ—¶å€™ï¼ŒåŠ å™ªå¯¹ä¸‹æ¸¸æ¨¡å‹çš„å­¦ä¹ å¯èƒ½ä¼šæ˜¯æ¯”è¾ƒé‡è¦çš„ä¸€ä¸ªå› ç´ ã€‚é™¤äº†ä¸Šè¿°ä¸¤ç¯‡æ–‡ç« ï¼Œç±»ä¼¼çš„è§‚ç‚¹åœ¨ELECTRAçš„å·¥ä½œä¸­ä¹Ÿå¯ä»¥çœ‹åˆ°æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„åœ°æ–¹ã€‚åœ¨ä¸€äº›åœºæ™¯ä¸‹ï¼Œæ•°æ®çš„æ„é€ è¿‡ç¨‹æ˜¯æ¯”è¾ƒå›°éš¾çš„ä»»åŠ¡ã€‚å› ä¸ºï¼Œå½“å¼€å§‹æ„é€ æ•°æ®çš„æ—¶å€™ï¼Œå…¶å®å·²ç»å¼•å…¥äº†ä¸€å®šçš„å½’çº³åç½®ã€‚è¿™é‡ŒåŒ…æ‹¬ï¼šå¹³è¡¡æ€§ï¼Œéš¾æ˜“ç¨‹åº¦ä»¥åŠä¸ä»»åŠ¡å¼ºç›¸å…³çš„ä¸€äº›ä¿¡å·ç­‰ã€‚æ€»ä¹‹ï¼Œå½“æ¨¡å‹æ˜¯åœ¨è‡ªå·±æ„å»ºçš„æ•°æ®ä¸Šå»learnçš„æ—¶å€™ï¼Œå¯¹æ¨¡å‹åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¿æŒè­¦æƒ•åº”è¯¥æ²¡æœ‰é”™ï¼Œæœ¬è´¨ä¸Šè¿˜æ˜¯i.i.dçš„é—®é¢˜ã€‚

22.ã€ŠZEN: Pre-training Chinese Encoder Enhanced by N-gram Representationsã€‹

å·ç§°ç›®å‰ä¸ºæ­¢æœ€å¼ºä¸­æ–‡NLPé¢„è®­ç»ƒæ¨¡å‹ã€‚æ•´ä½“æ€è·¯ä¸Šå¯ä»¥ä»deep&wideç»“æ„æ¥ç†è§£ã€‚deepç»“æ„å’Œä¼ ç»Ÿçš„bertç±»ä¼¼ï¼Œwideç»“æ„ç”¨æ¥encode n-gramçš„ä¿¡æ¯ã€‚å…¶å®ï¼Œæä¾›äº†ç¼“è§£ä¸­æ–‡bertåœ¨è¯è¯­ç²’åº¦ä¸Šmodelingä¸è¶³çš„é—®é¢˜ã€‚

å»¶ä¼¸æ€è€ƒï¼šå¯¹è¯è¯­ç²’åº¦çš„è¯­ä¹‰å•å…ƒmodelingæ˜¯å…³äºä¸­æ–‡bertçš„è€é—®é¢˜äº†ã€‚ä¸€èˆ¬æœ‰ä¸¤ä¸ªæ€è·¯ï¼šç¬¬ä¸€ä¸ªæ€è·¯ï¼Œç›´æ¥åœ¨deepç»“æ„çš„inputç«¯æ·»åŠ embeddingä¿¡æ¯ï¼Œä½†æ˜¯è¿™æ ·çš„é—®é¢˜åœ¨äºå¯èƒ½ä¼šå¼•å…¥noiseä¿¡æ¯ï¼›ç¬¬äºŒä¸ªæ€è·¯ï¼Œä½¿ç”¨wideç»“æ„ï¼Œå•ç‹¬è®­ç»ƒè¯è¯­ç²’åº¦çš„embeddingï¼Œæœ€ååœ¨outputç«¯è¿›è¡Œä¿¡æ¯çš„èåˆã€‚è¿™é‡Œé€‰æ‹©äº†ç¬¬äºŒç§ï¼Œå…¶å®ä¹Ÿæ˜¯ä¸€ç§æ¯”è¾ƒgeneralçš„æ€è·¯ã€‚

è™½ç„¶ä¸æ¸…æ¥šåœ¨å“ªä¸ªç²’åº¦å»ºæ¨¡æ¯”è¾ƒåˆé€‚ï¼Œä½†æ˜¯ä»æœ€è¿‘çš„ä¸€äº›å·¥ä½œæ¥çœ‹ï¼Œèåˆä¸€äº›é«˜å±‚semanticä¿¡æ¯ï¼ˆå¤šç²’åº¦çš„ä¿¡æ¯ï¼‰ä¸æ˜¯ä¸€ä»¶å¾ˆåçš„äº‹æƒ…ã€‚ä¸ç®¡æ€æ ·ï¼Œæ•´ä½“ä¸Šæ˜¯æˆ‘å–œæ¬¢çš„æ€è·¯ï¼Œç®€å•æœ‰æ•ˆã€‚

å…·ä½“å›¾å¦‚ä¸‹ï¼š

![img_zen](https://wx4.sinaimg.cn/mw690/aba7d18bly1g8om2b1jsej20iv0gradb.jpg)

21.ã€ŠELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORSã€‹,ICLR2020

ä¸»è¦å†…å®¹ï¼šThorough experiments demonstrate this new pre-training task is more efï¬- cient than MLM because the model learns from all input tokens rather than just the small subset that was masked out.

å»¶ä¼¸æ€è€ƒï¼šå’Œnon-parallel style transferçš„å·¥ä½œç±»ä¼¼ã€‚ä¸è¿‡è¿™ç¯‡çš„ä¸»è¦ç›®çš„æ˜¯å­¦åˆ°ä¸€ä¸ªå¥½çš„MLMã€‚å›¾ç¤ºå¦‚ä¸‹ï¼š

![img_electra](https://wx3.sinaimg.cn/mw690/aba7d18bly1g8jwz090qcj20
